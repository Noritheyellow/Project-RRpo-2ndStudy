{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.signal_processing import *\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "DATA_PATH = '/root/Workspace/DataWarehouse/stMary_RRpo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6508 1800\n",
      "(6508, 1800) (6508,)\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(f'{DATA_PATH}/21_230518_resamp_sliced125_filt_patient_stmary.pickle.gzip', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(len(dataset), len(dataset[0][0]))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "pleths = []\n",
    "resps = []\n",
    "for ppg, rr in dataset:\n",
    "    pleths.append(ppg.astype(np.float64))\n",
    "    resps.append(rr)\n",
    "\n",
    "pleths = np.asarray(pleths)\n",
    "resps = np.asarray(resps)\n",
    "print(pleths.shape, resps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1800, 1) <class 'numpy.float64'>\n",
      "(5206, 1800, 1) (5206,)\n",
      "(1302, 1800, 1) (1302,)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_pleths = np.asarray([scaler.fit_transform(pleth.reshape(-1,1)) for pleth in pleths])\n",
    "print(scaled_pleths.shape, type(scaled_pleths[0][0][0]))\n",
    "\n",
    "ratio_tr = 0.8\n",
    "train_x, train_y = scaled_pleths[:int(len(scaled_pleths)*ratio_tr)], resps[:int(len(resps)*ratio_tr)]\n",
    "val_x, val_y = scaled_pleths[int(len(scaled_pleths)*ratio_tr):], resps[int(len(resps)*ratio_tr):]\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture: Unet\n",
    "\n",
    "- Unet 모델 구조를 이용한다.\n",
    "- Level을 너무 깊게 파지 말고 천천히 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:09:03.253546: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-25 11:09:03.293789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 11:09:03.966739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU Avaliable: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, Conv1DTranspose, MaxPooling1D, AveragePooling1D, Dense, BatchNormalization, Activation, Add, Flatten, Dropout, Concatenate\n",
    "print(f'Is GPU Avaliable: {tf.config.list_physical_devices(\"GPU\")}')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContractionBlock(Model):\n",
    "    def __init__(self, starting_filter=32, dropout=0, *args, **kwargs):\n",
    "        super(ContractionBlock, self).__init__(*args, **kwargs)\n",
    "        self.conv1a = Conv1D(filters=starting_filter, kernel_size=3, strides=1, padding='same')\n",
    "        self.conv1b = Conv1D(filters=starting_filter*2, kernel_size=3, strides=1, padding='same')\n",
    "        self.maxpool = MaxPooling1D(pool_size=2, strides=2, padding='same')\n",
    "        \n",
    "        if dropout!=0:\n",
    "            self.dropout = Dropout(rate=dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        feature_map = self.conv1a(inputs)\n",
    "        feature_map = Activation('relu')(feature_map)\n",
    "        feature_map = self.conv1b(feature_map)\n",
    "        feature_map = Activation('relu')(feature_map)\n",
    "        \n",
    "        if self.dropout is not None:\n",
    "            feature_map = self.dropout(feature_map)\n",
    "\n",
    "        outputs = self.maxpool(feature_map)\n",
    "\n",
    "        return outputs, feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(Model):\n",
    "    def __init__(self, starting_filter=512, dropout=0, *args, **kwargs):\n",
    "        super(BottleNeckBlock, self).__init__(*args, **kwargs)\n",
    "        self.conv1a = Conv1D(filters=starting_filter, kernel_size=3, strides=1, padding='same')\n",
    "        self.conv1b = Conv1D(filters=starting_filter*2, kernel_size=3, strides=1, padding='same')\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        outputs = self.conv1a(inputs)\n",
    "        outputs = Activation('relu')(outputs)\n",
    "        outputs = self.conv1b(outputs)\n",
    "        outputs = Activation('relu')(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpansionBlock(Model):\n",
    "    def __init__(self, starting_filter=512, *args, **kwargs):\n",
    "        super(ExpansionBlock, self).__init__(*args, **kwargs)\n",
    "        self.conv1a = Conv1D(filters=starting_filter*2, kernel_size=3, strides=1, padding='same')\n",
    "        self.conv1b = Conv1D(filters=starting_filter, kernel_size=3, strides=1, padding='same')\n",
    "        self.upsampling = Conv1DTranspose(filters=starting_filter, kernel_size=2, strides=2, padding='same')\n",
    "    \n",
    "\n",
    "    def call(self, inputs, feature_map, training=None, mask=None):\n",
    "        x = self.upsampling(inputs)\n",
    "        x = Concatenate(axis=2)([x, feature_map])\n",
    "        x = self.conv1a(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output, feature_map = ContractionBlock(starting_filter=32, dropout=0.5)(val_x)\n",
    "# print(output.shape, feature_map.shape)\n",
    "# output = BottleNeckBlock(starting_filter=64, dropout=0.5)(output)\n",
    "# print(output.shape)\n",
    "# output = ExpansionBlock(starting_filter=64)(output, feature_map)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Unet, self).__init__(*args, **kwargs)\n",
    "        self.contraction_blocks = [ContractionBlock(starting_filter=f, dropout=d) for f, d in [(32,0), (64,0), (128,0), (256,0.5)]]\n",
    "        self.bottleneck_block = BottleNeckBlock(starting_filter=512, dropout=0.5)\n",
    "        self.expansion_blocks = [ExpansionBlock(starting_filter=f) for f in [512, 256, 128, 64]]\n",
    "        self.avgpool = AveragePooling1D(pool_size=2, strides=2, padding='valid')\n",
    "        self.maxpool = MaxPooling1D(pool_size=2, strides=2, padding='valid')\n",
    "\n",
    "        self.d100 = Dense(100, activation='relu')\n",
    "        self.d50 = Dense(50, activation='relu')\n",
    "        self.d10 = Dense(10, activation='relu')\n",
    "        self.d1 = Dense(1)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = inputs\n",
    "\n",
    "        x, feature_map1 = self.contraction_blocks[1](x)\n",
    "        x, feature_map2 = self.contraction_blocks[2](x)\n",
    "        x, feature_map3 = self.contraction_blocks[3](x)\n",
    "        # x, feature_map4 = self.contraction_blocks[3](x)\n",
    "\n",
    "        x = self.bottleneck_block(x)\n",
    "\n",
    "        x = self.expansion_blocks[0](x, feature_map3)\n",
    "        x = self.expansion_blocks[1](x, feature_map2)\n",
    "        x = self.expansion_blocks[2](x, feature_map1)\n",
    "        # x = self.expansion_blocks[3](x, feature_map1)\n",
    "\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = Flatten()(x)\n",
    "        x = self.d100(x)\n",
    "        x = self.d50(x)\n",
    "        x = self.d10(x)\n",
    "        \n",
    "        return self.d1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "kf = KFold(n_splits=5)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    # ModelCheckpoint('../models/230522-Resnet', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "model = Unet()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    # optimizer=tf.keras.optimizers.SGD(learning_rate=LR, momentum=0.9, weight_decay=0.0001),\n",
    "    loss=keras.losses.MeanAbsoluteError(),\n",
    "    metrics=keras.metrics.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 10:26:54.608499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5206]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 16.9733 - mean_absolute_error: 16.9733"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 10:27:21.872416: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [1302]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 30s 1s/step - loss: 16.9733 - mean_absolute_error: 16.9733 - val_loss: 19.7608 - val_mean_absolute_error: 19.7608 - lr: 0.0100\n",
      "Epoch 2/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 16.0738 - mean_absolute_error: 16.0738 - val_loss: 14.7659 - val_mean_absolute_error: 14.7659 - lr: 0.0100\n",
      "Epoch 3/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 13.9460 - mean_absolute_error: 13.9460 - val_loss: 12.6989 - val_mean_absolute_error: 12.6989 - lr: 0.0100\n",
      "Epoch 4/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 11.8629 - mean_absolute_error: 11.8629 - val_loss: 10.6290 - val_mean_absolute_error: 10.6290 - lr: 0.0100\n",
      "Epoch 5/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 9.8328 - mean_absolute_error: 9.8328 - val_loss: 8.6948 - val_mean_absolute_error: 8.6948 - lr: 0.0100\n",
      "Epoch 6/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 8.0073 - mean_absolute_error: 8.0073 - val_loss: 7.0692 - val_mean_absolute_error: 7.0692 - lr: 0.0100\n",
      "Epoch 7/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 6.6059 - mean_absolute_error: 6.6059 - val_loss: 5.9532 - val_mean_absolute_error: 5.9532 - lr: 0.0100\n",
      "Epoch 8/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 5.6828 - mean_absolute_error: 5.6828 - val_loss: 5.2774 - val_mean_absolute_error: 5.2774 - lr: 0.0100\n",
      "Epoch 9/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 5.1462 - mean_absolute_error: 5.1462 - val_loss: 4.9103 - val_mean_absolute_error: 4.9103 - lr: 0.0100\n",
      "Epoch 10/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.8385 - mean_absolute_error: 4.8385 - val_loss: 4.7292 - val_mean_absolute_error: 4.7292 - lr: 0.0100\n",
      "Epoch 11/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.6714 - mean_absolute_error: 4.6714 - val_loss: 4.6316 - val_mean_absolute_error: 4.6316 - lr: 0.0100\n",
      "Epoch 12/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.5967 - mean_absolute_error: 4.5967 - val_loss: 4.6057 - val_mean_absolute_error: 4.6057 - lr: 0.0100\n",
      "Epoch 13/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.5546 - mean_absolute_error: 4.5546 - val_loss: 4.5830 - val_mean_absolute_error: 4.5830 - lr: 0.0100\n",
      "Epoch 14/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.5175 - mean_absolute_error: 4.5175 - val_loss: 4.5748 - val_mean_absolute_error: 4.5748 - lr: 0.0100\n",
      "Epoch 15/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.5076 - mean_absolute_error: 4.5076 - val_loss: 4.5784 - val_mean_absolute_error: 4.5784 - lr: 0.0100\n",
      "Epoch 16/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.5044 - mean_absolute_error: 4.5044 - val_loss: 4.5813 - val_mean_absolute_error: 4.5813 - lr: 0.0100\n",
      "Epoch 17/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.5015 - mean_absolute_error: 4.5015 - val_loss: 4.5841 - val_mean_absolute_error: 4.5841 - lr: 0.0100\n",
      "Epoch 18/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4985 - mean_absolute_error: 4.4985 - val_loss: 4.5869 - val_mean_absolute_error: 4.5869 - lr: 0.0100\n",
      "Epoch 19/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4956 - mean_absolute_error: 4.4956 - val_loss: 4.5897 - val_mean_absolute_error: 4.5897 - lr: 0.0100\n",
      "Epoch 20/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4930 - mean_absolute_error: 4.4930 - val_loss: 4.5914 - val_mean_absolute_error: 4.5914 - lr: 1.0000e-03\n",
      "Epoch 21/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4923 - mean_absolute_error: 4.4923 - val_loss: 4.5919 - val_mean_absolute_error: 4.5919 - lr: 1.0000e-03\n",
      "Epoch 22/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4920 - mean_absolute_error: 4.4920 - val_loss: 4.5922 - val_mean_absolute_error: 4.5922 - lr: 1.0000e-03\n",
      "Epoch 23/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4917 - mean_absolute_error: 4.4917 - val_loss: 4.5924 - val_mean_absolute_error: 4.5924 - lr: 1.0000e-03\n",
      "Epoch 24/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4914 - mean_absolute_error: 4.4914 - val_loss: 4.5927 - val_mean_absolute_error: 4.5927 - lr: 1.0000e-03\n",
      "Epoch 25/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4911 - mean_absolute_error: 4.4911 - val_loss: 4.5929 - val_mean_absolute_error: 4.5929 - lr: 1.0000e-04\n",
      "Epoch 26/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4911 - mean_absolute_error: 4.4911 - val_loss: 4.5929 - val_mean_absolute_error: 4.5929 - lr: 1.0000e-04\n",
      "Epoch 27/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4910 - mean_absolute_error: 4.4910 - val_loss: 4.5930 - val_mean_absolute_error: 4.5930 - lr: 1.0000e-04\n",
      "Epoch 28/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4910 - mean_absolute_error: 4.4910 - val_loss: 4.5930 - val_mean_absolute_error: 4.5930 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.4910 - mean_absolute_error: 4.4910 - val_loss: 4.5930 - val_mean_absolute_error: 4.5930 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(BATCH_SIZE)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(history.history['val_loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(128)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(128)\n",
    "\n",
    "model = keras.models.load_model('../models/230524-Unet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:10:58.532212: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5206]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-25 11:10:59.458759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 6s 81ms/step\n",
      "(5206, 1) (5206,)\n",
      "0.5693018697474125 ± 0.9266279749787993\n"
     ]
    }
   ],
   "source": [
    "pred_y = model.predict(train_dataset)\n",
    "print(pred_y.shape, train_y.shape)\n",
    "abs_err = abs(train_y.reshape(-1,1) - pred_y)\n",
    "print(f'{np.mean(abs_err)} ± {np.std(abs_err)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/11 [=======>......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:11:16.379412: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [1302]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 97ms/step\n",
      "(1302, 1) (1302,)\n",
      "1.0132102955321562 ± 1.1088444677417446\n"
     ]
    }
   ],
   "source": [
    "pred_y = model.predict(val_dataset)\n",
    "print(pred_y.shape, val_y.shape)\n",
    "abs_err = abs(val_y.reshape(-1,1) - pred_y)\n",
    "print(f'{np.mean(abs_err)} ± {np.std(abs_err)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
