{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = '/root/Workspace/DataWarehouse/stMary_RRpo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6508 1800\n",
      "(6508, 1800) (6508,)\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(f'{DATA_PATH}/21_230518_resamp_sliced125_filt_patient_stmary.pickle.gzip', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(len(dataset), len(dataset[0][0]))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "pleths = []\n",
    "resps = []\n",
    "for ppg, rr in dataset:\n",
    "    pleths.append(ppg.astype(np.float64))\n",
    "    resps.append(rr)\n",
    "\n",
    "pleths = np.asarray(pleths)\n",
    "resps = np.asarray(resps)\n",
    "print(pleths.shape, resps.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1800, 1) <class 'numpy.float64'>\n",
      "(5206, 1800, 1) (5206,)\n",
      "(1302, 1800, 1) (1302,)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_pleths = np.asarray([scaler.fit_transform(pleth.reshape(-1,1)) for pleth in pleths])\n",
    "print(scaled_pleths.shape, type(scaled_pleths[0][0][0]))\n",
    "\n",
    "ratio_tr = 0.8\n",
    "train_x, train_y = scaled_pleths[:int(len(scaled_pleths)*ratio_tr)], resps[:int(len(resps)*ratio_tr)]\n",
    "val_x, val_y = scaled_pleths[int(len(scaled_pleths)*ratio_tr):], resps[int(len(resps)*ratio_tr):]\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture: ResNet 34 Layer\n",
    "\n",
    "- ResNet 모델 구조는 [논문](https://arxiv.org/pdf/1512.03385.pdf)을 참고하였다. 다만 해당 논문은 ImageNet의 데이터를 실행시키기 위한 것으로 조금의 조정이 필요한데 이를 변경하였을 때 그 내용을 논문에 기재할 필요가 있는지 의문이다.\n",
    "- 또는 만약 기재해야 한다면 내가 전부터 참고했던 [Bian의 논문](https://ieeexplore.ieee.org/document/9176231)을 참고해도 될 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 16:26:24.968220: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-19 16:26:25.008626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 16:26:25.678680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU Avaliable: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Dense, BatchNormalization, Activation, Add, Flatten\n",
    "print(f'Is GPU Avaliable: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(Model):\n",
    "    def __init__(self, filters, kernel_size, strides, identity_mapping=None, *args, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(*args, **kwargs)\n",
    "        self.conv1 = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides[0], padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "\n",
    "        self.conv2 = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides[1], padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "\n",
    "        self.identity_mapping = identity_mapping\n",
    "        self.conv_identity = Conv1D(filters=filters, kernel_size=1, strides=strides[0], padding='same')\n",
    "        \n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        identity = inputs\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = Activation('relu')(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        # 448, 64 / 224, 128\n",
    "        if self.identity_mapping:\n",
    "            identity = self.conv_identity(inputs)\n",
    "            # print(inputs.shape, identity.shape)\n",
    "\n",
    "        x = Add()([x, identity])\n",
    "        return Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ResNet34, self).__init__(*args, **kwargs)\n",
    "        self.conv1 = Conv1D(filters=64, kernel_size=7, strides=2)\n",
    "        self.max1d = MaxPooling1D(pool_size=3, strides=2)\n",
    "        self.resnet_block1 = [ResidualBlock(64, 3, (1,1)) for i in range(3)]\n",
    "        \n",
    "        self.resnet_block2_entry = ResidualBlock(128, 3, (2,1), identity_mapping=True)\n",
    "        self.resnet_block2 = [ResidualBlock(128, 3, (1,1)) for i in range(3)]\n",
    "\n",
    "        self.resnet_block3_entry = ResidualBlock(256, 3, (2,1), identity_mapping=True)\n",
    "        self.resnet_block3 = [ResidualBlock(256, 3, (1,1)) for i in range(5)]\n",
    "\n",
    "        self.resnet_block4_entry = ResidualBlock(512, 3, (2,1), identity_mapping=True)\n",
    "        self.resnet_block4 = [ResidualBlock(512, 3, (1,1)) for i in range(2)]\n",
    "\n",
    "        self.avg1d = AveragePooling1D(strides=2, padding='same')\n",
    "        self.flatten = Flatten()\n",
    "        self.d100 = Dense(100, activation='relu')\n",
    "        self.d50 = Dense(50, activation='relu')\n",
    "        self.d10 = Dense(10, activation='relu')\n",
    "        self.d1 = Dense(1)\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.max1d(x)\n",
    "\n",
    "        for block in self.resnet_block1:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        x = self.resnet_block2_entry(x, training=training)\n",
    "        for block in self.resnet_block2:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        x = self.resnet_block3_entry(x, training=training)\n",
    "        for block in self.resnet_block3:\n",
    "            x = block(x, training=training)\n",
    "        \n",
    "        x = self.resnet_block4_entry(x, training=training)\n",
    "        for block in self.resnet_block4:\n",
    "            x = block(x, training=training)\n",
    "        \n",
    "        \n",
    "        x = self.avg1d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d100(x)\n",
    "        x = self.d50(x)\n",
    "        x = self.d10(x)\n",
    "        return self.d1(x)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "kf = KFold(n_splits=5)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    # ModelCheckpoint('../models/230518-RRpo-4B', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "model = ResNet34()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=LR, momentum=0.9, weight_decay=0.0001),\n",
    "    loss=keras.losses.MeanAbsoluteError(),\n",
    "    metrics=keras.metrics.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 16:53:12.566689: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5206]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - ETA: 0s - loss: 4.4717 - mean_absolute_error: 4.4717"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 16:53:33.442851: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [1302]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 23s 118ms/step - loss: 4.4717 - mean_absolute_error: 4.4717 - val_loss: 9.5981 - val_mean_absolute_error: 9.5981 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "82/82 [==============================] - 8s 94ms/step - loss: 2.8941 - mean_absolute_error: 2.8941 - val_loss: 4.6020 - val_mean_absolute_error: 4.6020 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "82/82 [==============================] - 8s 93ms/step - loss: 2.3564 - mean_absolute_error: 2.3564 - val_loss: 4.8482 - val_mean_absolute_error: 4.8482 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "82/82 [==============================] - 8s 94ms/step - loss: 2.8709 - mean_absolute_error: 2.8709 - val_loss: 5.3002 - val_mean_absolute_error: 5.3002 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 2.2508 - mean_absolute_error: 2.2508 - val_loss: 5.0775 - val_mean_absolute_error: 5.0775 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 2.0020 - mean_absolute_error: 2.0020 - val_loss: 2.8320 - val_mean_absolute_error: 2.8320 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.7332 - mean_absolute_error: 1.7332 - val_loss: 4.5589 - val_mean_absolute_error: 4.5589 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.8614 - mean_absolute_error: 1.8614 - val_loss: 4.8910 - val_mean_absolute_error: 4.8910 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.9213 - mean_absolute_error: 1.9213 - val_loss: 2.3288 - val_mean_absolute_error: 2.3288 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 2.5413 - mean_absolute_error: 2.5413 - val_loss: 3.4532 - val_mean_absolute_error: 3.4532 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "82/82 [==============================] - 8s 94ms/step - loss: 1.4288 - mean_absolute_error: 1.4288 - val_loss: 3.7871 - val_mean_absolute_error: 3.7871 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 1.9783 - mean_absolute_error: 1.9783 - val_loss: 4.2990 - val_mean_absolute_error: 4.2990 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.9304 - mean_absolute_error: 1.9304 - val_loss: 2.0543 - val_mean_absolute_error: 2.0543 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 1.9288 - mean_absolute_error: 1.9288 - val_loss: 1.5222 - val_mean_absolute_error: 1.5222 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.9996 - mean_absolute_error: 1.9996 - val_loss: 7.8792 - val_mean_absolute_error: 7.8792 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 2.8445 - mean_absolute_error: 2.8445 - val_loss: 3.0011 - val_mean_absolute_error: 3.0011 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 1.7350 - mean_absolute_error: 1.7350 - val_loss: 1.9382 - val_mean_absolute_error: 1.9382 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.6726 - mean_absolute_error: 1.6726 - val_loss: 3.3690 - val_mean_absolute_error: 3.3690 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 2.0409 - mean_absolute_error: 2.0409 - val_loss: 2.0193 - val_mean_absolute_error: 2.0193 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 1.3574 - mean_absolute_error: 1.3574 - val_loss: 1.0321 - val_mean_absolute_error: 1.0321 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 1.1094 - mean_absolute_error: 1.1094 - val_loss: 1.1032 - val_mean_absolute_error: 1.1032 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.0179 - mean_absolute_error: 1.0179 - val_loss: 1.2133 - val_mean_absolute_error: 1.2133 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.0415 - mean_absolute_error: 1.0415 - val_loss: 1.1681 - val_mean_absolute_error: 1.1681 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 1.2014 - mean_absolute_error: 1.2014 - val_loss: 0.8454 - val_mean_absolute_error: 0.8454 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 1.0854 - mean_absolute_error: 1.0854 - val_loss: 0.8671 - val_mean_absolute_error: 0.8671 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.9823 - mean_absolute_error: 0.9823 - val_loss: 0.8427 - val_mean_absolute_error: 0.8427 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.8888 - mean_absolute_error: 0.8888 - val_loss: 1.2620 - val_mean_absolute_error: 1.2620 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.8877 - mean_absolute_error: 0.8877 - val_loss: 1.7726 - val_mean_absolute_error: 1.7726 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.7742 - mean_absolute_error: 0.7742 - val_loss: 1.4827 - val_mean_absolute_error: 1.4827 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.9676 - mean_absolute_error: 0.9676 - val_loss: 1.6834 - val_mean_absolute_error: 1.6834 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 1.0630 - mean_absolute_error: 1.0630 - val_loss: 1.5583 - val_mean_absolute_error: 1.5583 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 1.5120 - mean_absolute_error: 1.5120 - val_loss: 0.9005 - val_mean_absolute_error: 0.9005 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.6501 - mean_absolute_error: 0.6501 - val_loss: 0.8486 - val_mean_absolute_error: 0.8486 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 0.6177 - mean_absolute_error: 0.6177 - val_loss: 0.8355 - val_mean_absolute_error: 0.8355 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.5901 - mean_absolute_error: 0.5901 - val_loss: 0.8312 - val_mean_absolute_error: 0.8312 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.5663 - mean_absolute_error: 0.5663 - val_loss: 0.8301 - val_mean_absolute_error: 0.8301 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.5426 - mean_absolute_error: 0.5426 - val_loss: 0.8261 - val_mean_absolute_error: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.5232 - mean_absolute_error: 0.5232 - val_loss: 0.8219 - val_mean_absolute_error: 0.8219 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.5019 - mean_absolute_error: 0.5019 - val_loss: 0.8206 - val_mean_absolute_error: 0.8206 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.4996 - mean_absolute_error: 0.4996 - val_loss: 0.8206 - val_mean_absolute_error: 0.8206 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.4715 - mean_absolute_error: 0.4715 - val_loss: 0.8188 - val_mean_absolute_error: 0.8188 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.4462 - mean_absolute_error: 0.4462 - val_loss: 0.8185 - val_mean_absolute_error: 0.8185 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 0.4459 - mean_absolute_error: 0.4459 - val_loss: 0.8203 - val_mean_absolute_error: 0.8203 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.4310 - mean_absolute_error: 0.4310 - val_loss: 0.8176 - val_mean_absolute_error: 0.8176 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.4291 - mean_absolute_error: 0.4291 - val_loss: 0.8138 - val_mean_absolute_error: 0.8138 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.3950 - mean_absolute_error: 0.3950 - val_loss: 0.8123 - val_mean_absolute_error: 0.8123 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 0.3835 - mean_absolute_error: 0.3835 - val_loss: 0.8121 - val_mean_absolute_error: 0.8121 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 0.3707 - mean_absolute_error: 0.3707 - val_loss: 0.8158 - val_mean_absolute_error: 0.8158 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.3440 - mean_absolute_error: 0.3440 - val_loss: 0.8223 - val_mean_absolute_error: 0.8223 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.3497 - mean_absolute_error: 0.3497 - val_loss: 0.8250 - val_mean_absolute_error: 0.8250 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.3601 - mean_absolute_error: 0.3601 - val_loss: 0.8248 - val_mean_absolute_error: 0.8248 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.3580 - mean_absolute_error: 0.3580 - val_loss: 0.8266 - val_mean_absolute_error: 0.8266 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.4339 - mean_absolute_error: 0.4339 - val_loss: 0.8098 - val_mean_absolute_error: 0.8098 - lr: 1.0000e-06\n",
      "Epoch 54/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2910 - mean_absolute_error: 0.2910 - val_loss: 0.8094 - val_mean_absolute_error: 0.8094 - lr: 1.0000e-06\n",
      "Epoch 55/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2871 - mean_absolute_error: 0.2871 - val_loss: 0.8091 - val_mean_absolute_error: 0.8091 - lr: 1.0000e-06\n",
      "Epoch 56/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2844 - mean_absolute_error: 0.2844 - val_loss: 0.8090 - val_mean_absolute_error: 0.8090 - lr: 1.0000e-06\n",
      "Epoch 57/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2819 - mean_absolute_error: 0.2819 - val_loss: 0.8089 - val_mean_absolute_error: 0.8089 - lr: 1.0000e-06\n",
      "Epoch 58/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2797 - mean_absolute_error: 0.2797 - val_loss: 0.8089 - val_mean_absolute_error: 0.8089 - lr: 1.0000e-06\n",
      "Epoch 59/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2778 - mean_absolute_error: 0.2778 - val_loss: 0.8088 - val_mean_absolute_error: 0.8088 - lr: 1.0000e-06\n",
      "Epoch 60/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2747 - mean_absolute_error: 0.2747 - val_loss: 0.8090 - val_mean_absolute_error: 0.8090 - lr: 1.0000e-06\n",
      "Epoch 61/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2728 - mean_absolute_error: 0.2728 - val_loss: 0.8089 - val_mean_absolute_error: 0.8089 - lr: 1.0000e-06\n",
      "Epoch 62/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2713 - mean_absolute_error: 0.2713 - val_loss: 0.8089 - val_mean_absolute_error: 0.8089 - lr: 1.0000e-06\n",
      "Epoch 63/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2683 - mean_absolute_error: 0.2683 - val_loss: 0.8089 - val_mean_absolute_error: 0.8089 - lr: 1.0000e-06\n",
      "Epoch 64/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2613 - mean_absolute_error: 0.2613 - val_loss: 0.8090 - val_mean_absolute_error: 0.8090 - lr: 1.0000e-07\n",
      "Epoch 65/100\n",
      "82/82 [==============================] - 8s 94ms/step - loss: 0.2610 - mean_absolute_error: 0.2610 - val_loss: 0.8091 - val_mean_absolute_error: 0.8091 - lr: 1.0000e-07\n",
      "Epoch 66/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2607 - mean_absolute_error: 0.2607 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-07\n",
      "Epoch 67/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2605 - mean_absolute_error: 0.2605 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-07\n",
      "Epoch 68/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2603 - mean_absolute_error: 0.2603 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-07\n",
      "Epoch 69/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2591 - mean_absolute_error: 0.2591 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-08\n",
      "Epoch 70/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2590 - mean_absolute_error: 0.2590 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-08\n",
      "Epoch 71/100\n",
      "82/82 [==============================] - 8s 97ms/step - loss: 0.2590 - mean_absolute_error: 0.2590 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-08\n",
      "Epoch 72/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2590 - mean_absolute_error: 0.2590 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-08\n",
      "Epoch 73/100\n",
      "82/82 [==============================] - 8s 96ms/step - loss: 0.2590 - mean_absolute_error: 0.2590 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-08\n",
      "Epoch 74/100\n",
      "82/82 [==============================] - 8s 95ms/step - loss: 0.2589 - mean_absolute_error: 0.2589 - val_loss: 0.8092 - val_mean_absolute_error: 0.8092 - lr: 1.0000e-09\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(BATCH_SIZE)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.808845579624176"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
