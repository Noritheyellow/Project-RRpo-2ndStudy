{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "`20_230517_sliced_filt_patient_stmary.npy`를 이용해서 ResNet 모델을 학습시킨다. 보다 좀 더 독자적인 모델로 발전시키는 것도 좋을 것 같다.\n",
    "\n",
    "**1. Normalization**\n",
    "- 불러온 PLETH 데이터 각각에 대해서 `min-max` 정규화를 실행한다.\n",
    "\n",
    "**2. Model Configuration**\n",
    "- `Dataset split`: train(80%):validation(20%) 5-Fold CV\n",
    "- `Testset`: BIDMC dataset\n",
    "- `Batch size`: [4, 16, 32, 64, 128, 256] \n",
    "- `Epochs`: 100\n",
    "- `Callbacks`: [Earlystopping, ModelCheckpoint]\n",
    "- `Optimizer`: Adam\n",
    "- `Loss function`: [MAE, MSE, RMSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(489, 2)\n",
      "[[array([ 1.34310827e-01, -1.72755175e+01, -3.50188641e+01, ...,\n",
      "          2.53006996e+02,  3.04690014e+02,  3.49826494e+02])     18]\n",
      " [array([-173.89644469, -191.88503338, -207.42843855, ..., -614.50780984,\n",
      "         -617.1630912 , -615.95674291])\n",
      "  17]\n",
      " [array([-61.06533745, -44.254553  , -28.86251007, ..., -43.80296885,\n",
      "         -55.05317591, -66.50906096])                                 17]\n",
      " [array([-231.89353892, -248.54359463, -265.19371548, ..., -394.30426383,\n",
      "         -403.98687518, -414.40740483])\n",
      "  16]\n",
      " [array([387.72504662, 418.03740933, 440.7437886 , ...,  14.56412795,\n",
      "          -3.47130216, -20.03924253])                                 17]]\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../../DataWarehouse/stMary_RRpo/20_230517_sliced_filt_patient_stmary.npy', allow_pickle=True)\n",
    "print(dataset.shape)\n",
    "print(dataset[:5][:])\n",
    "print(len(dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(489, 2)\n"
     ]
    }
   ],
   "source": [
    "random.seed(40)\n",
    "random.shuffle(dataset)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(489,) (489,)\n"
     ]
    }
   ],
   "source": [
    "pleths = dataset[:,0]\n",
    "resps = dataset[:,1].astype(np.float64)\n",
    "print(pleths.shape, resps.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489, 7500, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_pleths = np.asarray([scaler.fit_transform(pleth.reshape(-1,1)) for pleth in pleths], dtype=np.float64)\n",
    "scaled_pleths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 7500, 1) (391,)\n",
      "(98, 7500, 1) (98,)\n"
     ]
    }
   ],
   "source": [
    "ratio_tr = 0.8\n",
    "train_x, train_y = scaled_pleths[:int(len(scaled_pleths)*ratio_tr)], resps[:int(len(resps)*ratio_tr)]\n",
    "val_x, val_y = scaled_pleths[int(len(scaled_pleths)*ratio_tr):], resps[int(len(resps)*ratio_tr):]\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture: ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:28:15.688455: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-17 19:28:15.734952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 19:28:16.402265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU Avaliable: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D, Add, LeakyReLU, MaxPooling1D, Flatten, Dense, BatchNormalization, Activation\n",
    "print(f'Is GPU Avaliable: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        self.block_conv1 = Conv1D(filters, kernel_size, strides=strides, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.block_conv2 = Conv1D(filters, kernel_size, strides=1, padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.block_conv3 = Conv1D(filters, kernel_size, strides=1, padding='same')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.block_add = Add()\n",
    "        self.leaky_relu = LeakyReLU()\n",
    "\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        x0 = self.block_conv1(input)\n",
    "        x0 = self.bn1(x0, training=training)\n",
    "        x0 = Activation('relu')(x0)\n",
    "        # x0 = self.leaky_relu(x0)\n",
    "\n",
    "        x1 = self.block_conv2(x0)\n",
    "        x1 = self.bn1(x1, training=training)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        # x1 = self.leaky_relu(x1)\n",
    "        x1 = self.block_conv3(x1)\n",
    "        x1 = self.bn1(x1, training=training)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        # x1 = self.leaky_relu(x1)\n",
    "\n",
    "        x = self.block_add([x0, x1])\n",
    "        # return self.leaky_relu(x)\n",
    "        return Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet_block = [ResnetIdentityBlock(filters=6*(2**i), kernel_size=3, strides=2) for i in range(5)]\n",
    "        self.max1d = MaxPooling1D(strides=2, padding='same')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense0 = Dense(20, activation='relu')\n",
    "        self.dense1 = Dense(10, activation='relu')\n",
    "        self.dense2 = Dense(1)\n",
    "        self.leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # @tf.function\n",
    "    def call(self, input, training=False):\n",
    "        x = input\n",
    "        for i in range(5):\n",
    "            x = self.resnet_block[i](x, training)\n",
    "\n",
    "        x = self.max1d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense0(x)\n",
    "        # x = self.leaky_relu(x)\n",
    "        x = self.dense1(x)\n",
    "        # x = self.leaky_relu(x)\n",
    "\n",
    "        return self.dense2(x)\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        '''\n",
    "            train_step은 fit()를 타고 넘어온 data를 unpack하여 학습을 진행한다.\n",
    "            compile()의 loss 함수를 토대로 self.compiled_loss를 진행한다.\n",
    "            self.compiled_metrics는 compile()의 metrics를 토대로 진행한다.\n",
    "        '''\n",
    "        x, y = data\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        '''\n",
    "            Evaluation을 위한 Custom function이다.\n",
    "        '''\n",
    "        x, y = data\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrpo_model = ResNet()\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = [4, 16, 32, 64, 128, 256]\n",
    "kf = KFold(n_splits=5)\n",
    "rrpo_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(),\n",
    ")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "    tf.keras.callbacks.ModelCheckpoint('../models/230517-RRpo-batch4/', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(BATCH_SIZE[4])\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(BATCH_SIZE[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:40:32.737642: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [391]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 6.5337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:40:45.041305: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [98]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 13s 43ms/step - loss: 6.5337 - val_loss: 8.3159\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 3.7992 - val_loss: 3.6563\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 3.2971 - val_loss: 3.6492\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 2.9075 - val_loss: 3.7576\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 2.5222 - val_loss: 3.7446\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 2.1806 - val_loss: 3.9230\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.9432 - val_loss: 3.8383\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 1.8099 - val_loss: 4.1486\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.6823 - val_loss: 4.6194\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.7521 - val_loss: 4.3705\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 1.5120 - val_loss: 3.7893\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1.2663 - val_loss: 3.9485\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.2682 - val_loss: 4.1531\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.2138 - val_loss: 3.7062\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 1.1989 - val_loss: 3.8135\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.4463 - val_loss: 4.0289\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 1.1999 - val_loss: 3.8550\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.3426 - val_loss: 4.1472\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.4811 - val_loss: 4.2180\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.6395 - val_loss: 4.7030\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.8419 - val_loss: 5.0251\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 1.4108 - val_loss: 5.1704\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 1.4490 - val_loss: 5.1826\n",
      "Epoch 1/100\n",
      " 5/98 [>.............................] - ETA: 2s - loss: 2.2297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:41:58.057687: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [391]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 3s 32ms/step - loss: 1.6779 - val_loss: 5.5332\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:42:01.028469: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [98]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 3s 32ms/step - loss: 1.4206 - val_loss: 5.4634\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 1.1047 - val_loss: 5.3962\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.0186 - val_loss: 5.5450\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.9449 - val_loss: 5.6222\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 1.3111 - val_loss: 6.4938\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 1.5292 - val_loss: 6.3869\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.4209 - val_loss: 6.2620\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.2625 - val_loss: 6.1401\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.2590 - val_loss: 6.0379\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.2030 - val_loss: 6.3400\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.0984 - val_loss: 5.9187\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 1.3221 - val_loss: 6.4414\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.1299 - val_loss: 6.1189\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.2566 - val_loss: 6.3793\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.1951 - val_loss: 5.9322\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.1506 - val_loss: 6.5683\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.8658 - val_loss: 6.1813\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 0.7116 - val_loss: 5.8672\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 0.9879 - val_loss: 6.0674\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.3896 - val_loss: 5.4235\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.0297 - val_loss: 5.5368\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 0.9116 - val_loss: 5.6937\n",
      "Epoch 1/100\n",
      " 5/98 [>.............................] - ETA: 2s - loss: 1.3834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:43:12.110794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [391]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 3s 34ms/step - loss: 1.1299 - val_loss: 4.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:43:15.219897: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [98]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 0.9654 - val_loss: 5.1834\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 1.0128 - val_loss: 5.4857\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.3168 - val_loss: 5.6632\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 1.2212 - val_loss: 6.0213\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.9735 - val_loss: 5.6592\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.8479 - val_loss: 5.1751\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 1.0248 - val_loss: 4.8487\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.9218 - val_loss: 4.7587\n",
      "Epoch 10/100\n",
      "42/98 [===========>..................] - ETA: 1s - loss: 0.8008"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((X_train, y_train))\u001b[39m.\u001b[39mbatch(BATCH_SIZE[\u001b[39m0\u001b[39m])\n\u001b[1;32m      7\u001b[0m val_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((X_val, y_val))\u001b[39m.\u001b[39mbatch(BATCH_SIZE[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m hist \u001b[39m=\u001b[39m rrpo_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     10\u001b[0m     train_dataset,\n\u001b[1;32m     11\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m     12\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     13\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39m# history.append(hist)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "for train_idx, val_idx in kf.split(scaled_pleths):\n",
    "    X_train, y_train = scaled_pleths[train_idx], resps[train_idx]\n",
    "    X_val, y_val = scaled_pleths[val_idx], resps[val_idx]\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE[0])\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE[0])\n",
    "\n",
    "    hist = rrpo_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_dataset,\n",
    "    )\n",
    "    # history.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        hist = rrpo_model.fit(\n",
    "            train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=val_dataset    \n",
    "        )\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "for train_idx, val_idx in kf.split(scaled_pleths):\n",
    "    X_train, y_train = scaled_pleths[train_idx], resps[train_idx]\n",
    "    X_val, y_val = scaled_pleths[val_idx], resps[val_idx]\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE[0])\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE[0])\n",
    "\n",
    "    \n",
    "    hist = rrpo_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_dataset,\n",
    "    )\n",
    "    history.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
