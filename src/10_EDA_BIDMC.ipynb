{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 18:32:13.815644: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-20 18:32:15.947391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 18:32:34.796732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU Avaliable: []\n",
      "Is GPU Avaliable: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 18:33:20.063312: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-09-20 18:33:20.063365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 331e9bf49250\n",
      "2023-09-20 18:33:20.063376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 331e9bf49250\n",
      "2023-09-20 18:33:20.063533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.89.2\n",
      "2023-09-20 18:33:20.063558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.89.2\n",
      "2023-09-20 18:33:20.063567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.89.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from model_src.DilatedResNet import DilatedResNet\n",
    "from model_src.BianResnet import BianResNet\n",
    "from model_src.LSTM import VanillaLSTM, CNNLSTM, BiLSTM, BiLSTMAttn\n",
    "from model_src.RespNet import RespNet\n",
    "\n",
    "DATA_PATH = '../../DataLake/bidmc_csv'\n",
    "DATA_PATH = '../../DataLake/Capnobase/data/csv'\n",
    "regex_bidmc = re.compile('bidmc_[0-9]+')\n",
    "regex_capno = re.compile('[0-9]{4}_8min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIDMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidmc_id = sorted(list(set([regex_bidmc.match(filename.name).group() for filename in os.scandir(DATA_PATH)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 125\n",
    "window_size = fs * 60 # 7500\n",
    "shift = int(window_size/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleth = []; resp = []\n",
    "for sid in bidmc_id:\n",
    "    pleth_csv = pd.read_csv(f'{DATA_PATH}/{sid}_Signals.csv', names=['Time [s]', ' PLETH'])\n",
    "    resp_csv = pd.read_csv(f'{DATA_PATH}/{sid}_Numerics.csv', names=['Time [s]', ' RESP'])\n",
    "    samp_pleth = np.expand_dims(np.array([pleth_csv[0+shift*i:window_size+shift*i][' PLETH'].values for i in range(int((len(pleth_csv)-window_size)/shift)+1)]), axis=-1)\n",
    "    samp_resp = np.array([round(np.mean(resp_csv[0+int(shift/fs)*i:int(window_size/fs)+int(shift/fs)*i][' RESP'])) for i in range(int((len(pleth_csv)-window_size)/shift)+1)]).reshape(-1,1)\n",
    "    pleth.append(samp_pleth)\n",
    "    resp.append(samp_resp)\n",
    "\n",
    "pleth = np.concatenate(pleth, axis=0)\n",
    "resp = np.concatenate(resp, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capnobase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(x, input):\n",
    "    x0 = int(np.floor(x))\n",
    "    y0 = input[x0]\n",
    "    x1 = int(np.ceil(x))\n",
    "    y1 = input[x1]\n",
    "    y = (y1-y0)*(x-x0) + y0\n",
    "    return y\n",
    "\n",
    "\n",
    "def signal_resample(input_signal, org_fs, new_fs, method='interpolation'):\n",
    "    output_signal = []\n",
    "    new_x = np.arange(0, len(input_signal), org_fs/new_fs)\n",
    "    \n",
    "    if method == 'interpolation': \n",
    "        interp = interpolation\n",
    "\n",
    "    for x in new_x:\n",
    "        y = interp(x, input_signal)\n",
    "        output_signal.append(y)\n",
    "\n",
    "    return np.asarray(output_signal)\n",
    "\n",
    "\n",
    "def generate_dataset(arg_pleths, arg_resps, fs=125, shift_factor=4):\n",
    "    \"\"\"\n",
    "    성모병원에서 수집된 데이터의 특성상 이러한 전처리를 진행해주어야 한다.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    dataset = []\n",
    "    window_size = fs * 60 # 7500\n",
    "    shift = int(window_size/shift_factor)\n",
    "    samples_len = len(arg_pleths)\n",
    "\n",
    "    cpy_resps = copy.deepcopy(arg_resps)\n",
    "    cpy_pleths = copy.deepcopy(arg_pleths)\n",
    "\n",
    "    for j in range(samples_len):\n",
    "        rr = cpy_resps[j]; ppg = cpy_pleths[j]\n",
    "\n",
    "        rr['offset'] = (rr['offset']-rr['offset'].min())/1000\n",
    "        size_lim = int(fs * np.ceil(rr['offset'].max()))\n",
    "        ppg = ppg[:size_lim]\n",
    "        shift_n_times = int((len(ppg)-window_size)/shift)+1\n",
    "\n",
    "        samp_rr = [len(rr.loc[ (rr['offset']>=0+(int(shift/fs)*i)) & ((rr['offset']<int(window_size/fs)+(int(shift/fs)*i))) ]) for i in range(shift_n_times)]\n",
    "        samp_ppg = [ppg[0+(shift*i):window_size+(shift*i)] for i in range(shift_n_times)]\n",
    "\n",
    "        for i in range(len(samp_ppg)):\n",
    "            temp = []\n",
    "            temp.append(samp_ppg[i])\n",
    "            temp.append(samp_rr[i])\n",
    "            dataset.append(temp)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocessing(targets, numtaps, cutoff, shift_factor, org_fs, new_fs, processes):\n",
    "    print('Extract PLETH/RESP')\n",
    "    pleths = [pd.read_csv(f'{DATA_PATH}/{sid}/pleth.csv', header=None, names=['sid', 'offset', 'pleth']).pleth.values for sid in targets.id.unique()]\n",
    "    resps = [pd.read_csv(f'{DATA_PATH}/{sid}/respirationTimeline.csv', header=None, names=['sid', 'offset']) for sid in targets.id.unique()]\n",
    "\n",
    "    # Before filtering: Check NaN\n",
    "    for pleth in pleths:\n",
    "        if any(np.isnan(pleth)):\n",
    "            print('check')\n",
    "\n",
    "    # Before filtering: Convert type as np.float32\n",
    "    pleths = list(map(lambda pleth: pleth.astype(np.float32), pleths))\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Filtering')\n",
    "    taps = signal.firwin(numtaps=numtaps, cutoff=cutoff, window='hamming', pass_zero=False, fs=org_fs)\n",
    "    pool = multiprocessing.Pool(processes=processes)\n",
    "    filtered_pleths = pool.starmap(signal.filtfilt, [(taps, 1.0, pleth) for pleth in pleths])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Windowing')\n",
    "    dataset = generate_dataset(filtered_pleths, resps, shift_factor=shift_factor)\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Resampling')\n",
    "    pool = multiprocessing.Pool(processes=processes)\n",
    "    result = pool.starmap(signal_resample, [(pleth[0], org_fs, new_fs) for pleth in dataset])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    new_patient = []\n",
    "    for i in range(len(dataset)):\n",
    "        temp = []\n",
    "        temp.append(result[i])\n",
    "        temp.append(dataset[i][1])\n",
    "        new_patient.append(temp)\n",
    "\n",
    "    return new_patient\n",
    "\n",
    "\n",
    "def prepare_modeling(dataset=None, batchsize=None):\n",
    "    print(f'Prepare modeling')\n",
    "    pleths = []\n",
    "    resps = []\n",
    "    for ppg, rr in dataset:\n",
    "        pleths.append(ppg.astype(np.float32))\n",
    "        resps.append(rr)\n",
    "    pleths = np.asarray(pleths)\n",
    "    resps = np.asarray(resps)\n",
    "    print(pleths.shape, resps.shape)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_pleths = np.asarray([scaler.fit_transform(pleth.reshape(-1,1)) for pleth in pleths])\n",
    "    print(scaled_pleths.shape, type(scaled_pleths[0][0][0]))\n",
    "\n",
    "    x, y = scaled_pleths[:], resps[:]\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(batchsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
