{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 15:46:59.030086: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-01 15:46:59.077510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 15:46:59.717015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from scipy import signal\n",
    "from itertools import starmap\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from model_src.Resnet import ResNet34\n",
    "from model_src.Unet import Unet\n",
    "from model_src.DilatedConv import RespDNN\n",
    "from model_src.BianResnet import ResNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = '../../DataLake/stMary'\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "DATALAKE_PATH = '../../DataLake/rrest-syn_csv'\n",
    "regex = re.compile('rrest-syn[0-9]+')\n",
    "syn_id = sorted(list(set([regex.match(filename.name).group() for filename in os.scandir(DATALAKE_PATH)])))\n",
    "fs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(arg_pleths, arg_resps, fs=125, shift_factor=4):\n",
    "    import copy\n",
    "    dataset = []\n",
    "    window_size = fs * 60 # 7500\n",
    "    shift = int(window_size/shift_factor) # 1875\n",
    "    samples_len = len(arg_pleths)\n",
    "\n",
    "    cpy_resps = copy.deepcopy(arg_resps)\n",
    "    cpy_pleths = copy.deepcopy(arg_pleths)\n",
    "\n",
    "    for i in range(samples_len):\n",
    "        rr = cpy_resps[i]; ppg = cpy_pleths[i]\n",
    "\n",
    "        rr['offset'] = (rr['offset']-rr['offset'].min())/1000\n",
    "        size_lim = int(fs * np.ceil(rr['offset'].max()))\n",
    "        ppg = ppg[:size_lim]\n",
    "        shift_n_times = int((len(ppg)-window_size)/shift)+1\n",
    "\n",
    "        samp_rr = [len(rr.loc[ (rr['offset']>=0+(int(shift/fs)*i)) & ((rr['offset']<int(window_size/fs)+(int(shift/fs)*i))) ]) for i in range(shift_n_times)]\n",
    "        samp_ppg = [ppg[0+(shift*i):window_size+(shift*i)] for i in range(shift_n_times)]\n",
    "\n",
    "        for i in range(len(samp_ppg)):\n",
    "            temp = []\n",
    "            temp.append(samp_ppg[i])\n",
    "            temp.append(samp_rr[i])\n",
    "            dataset.append(temp)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def interpolation(x, input):\n",
    "    x0 = int(np.floor(x))\n",
    "    y0 = input[x0]\n",
    "    x1 = int(np.ceil(x))\n",
    "    y1 = input[x1]\n",
    "    y = (y1-y0)*(x-x0) + y0\n",
    "    return y\n",
    "\n",
    "\n",
    "def signal_resample(input_signal, org_fs, new_fs, method='interpolation'):\n",
    "    output_signal = []\n",
    "    new_x = np.arange(0, len(input_signal), org_fs/new_fs)\n",
    "    \n",
    "    if method == 'interpolation': \n",
    "        interp = interpolation\n",
    "\n",
    "    for x in new_x:\n",
    "        y = interp(x, input_signal)\n",
    "        output_signal.append(y)\n",
    "\n",
    "    return np.asarray(output_signal)\n",
    "\n",
    "\n",
    "def preprocessing(targets=None):\n",
    "    print('Extract PLETH/RESP')\n",
    "    pleths = [pd.read_csv(f'{DATA_PATH}/{sid}/pleth.csv', header=None, names=['sid', 'offset', 'pleth']).pleth.values for sid in targets.id.unique()]\n",
    "    resps = [pd.read_csv(f'{DATA_PATH}/{sid}/respirationTimeline.csv', header=None, names=['sid', 'offset']) for sid in targets.id.unique()]\n",
    "\n",
    "    # Before filtering: Check NaN\n",
    "    for pleth in pleths:\n",
    "        if any(np.isnan(pleth)):\n",
    "            print('check')\n",
    "\n",
    "    # Before filtering: Convert type as np.int16\n",
    "    pleths = list(map(lambda pleth: pleth.astype(np.float32), pleths))\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Filtering')\n",
    "    taps = signal.firwin(numtaps=400, cutoff=[0.5, 8.0], window='hamming', pass_zero=False, fs=125)\n",
    "    w, h = signal.freqz(taps)\n",
    "    pool = multiprocessing.Pool(processes=40)\n",
    "    filtered_pleths = pool.starmap(signal.filtfilt, [(taps, 1.0, pleth) for pleth in pleths])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Windowing')\n",
    "    dataset = generate_dataset(filtered_pleths, resps, shift_factor=60)\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Resampling')\n",
    "    pool = multiprocessing.Pool(processes=40)\n",
    "    result = pool.starmap(signal_resample, [(pleth[0], 125, 30) for pleth in dataset])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    new_patient = []\n",
    "    for i in range(len(dataset)):\n",
    "        temp = []\n",
    "        temp.append(result[i])\n",
    "        temp.append(dataset[i][1])\n",
    "        new_patient.append(temp)\n",
    "\n",
    "    return new_patient\n",
    "\n",
    "\n",
    "def prepare_modeling(dataset=None, batchsize=None):\n",
    "    print(f'Prepare modeling')\n",
    "    pleths = []\n",
    "    resps = []\n",
    "    for ppg, rr in dataset:\n",
    "        pleths.append(ppg.astype(np.float32))\n",
    "        resps.append(rr)\n",
    "    pleths = np.asarray(pleths)\n",
    "    resps = np.asarray(resps)\n",
    "    print(pleths.shape, resps.shape)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_pleths = np.asarray([scaler.fit_transform(pleth.reshape(-1,1)) for pleth in pleths])\n",
    "    print(scaled_pleths.shape, type(scaled_pleths[0][0][0]))\n",
    "\n",
    "    x, y = scaled_pleths[:], resps[:]\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(batchsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic Dataset은 순수하게 학습 용도이다.\n",
    "\n",
    "이를 성모병원 SICU와 섞어 사용할 수 있는 이유는 이것이 인조 데이터셋이기 때문이다.\n",
    "\n",
    "1. 이는 다른 병원의 데이터셋의 경우 나타날 수 있는 Bias 가능성으로부터 자유롭다.\n",
    "\n",
    "2. 어느 병원이든지 이 인조 데이터셋 알고리즘을 통해서 데이터를 만들어 낼 수 있기 때문에 접근성이 좋다.\n",
    "\n",
    "3. 이는 모델의 과적합을 방지하는 역할을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled synthetic from 500hz to 125hz: (192, 15000)\n"
     ]
    }
   ],
   "source": [
    "temp_resp = []\n",
    "for i in range(len(syn_id)):\n",
    "    with open(f'{DATALAKE_PATH}/{syn_id[i]}_fix.txt') as f:\n",
    "        temp_resp.append(f.readlines()[-2])\n",
    "    \n",
    "temp_resp = np.asarray(temp_resp)\n",
    "\n",
    "exp_resp = re.compile(r'\\d+')\n",
    "syn_resp = list(map(lambda r: int(exp_resp.findall(r)[0]), temp_resp))\n",
    "\n",
    "syn_pleth = []\n",
    "for i in range(len(syn_id)):\n",
    "    sig_dat = pd.read_csv(f'{DATALAKE_PATH}/{syn_id[i]}_data.csv', header=None, names=['PLETH', 'ECG'])\n",
    "    syn_pleth.append(sig_dat.PLETH.values[500:60500]) # 성모병원도 약 2분 데이터이므로\n",
    "\n",
    "syn_pleth = np.asarray(syn_pleth)\n",
    "\n",
    "pool = multiprocessing.Pool(processes=40)\n",
    "syn_pleth125 = pool.starmap(signal_resample, [(p, 500, 125) for p in syn_pleth])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(f'Resampled synthetic from 500hz to 125hz: {np.asarray(syn_pleth125).shape}')\n",
    "\n",
    "# Before filtering: Check NaN\n",
    "for pleth in syn_pleth125:\n",
    "    if any(np.isnan(pleth)):\n",
    "        print('check')\n",
    "\n",
    "# Before filtering: Convert type as np.int16\n",
    "syn_pleth125 = list(map(lambda pleth: pleth.astype(np.float32), syn_pleth125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing commencing...\n",
      "> Filtering\n",
      "(192, 15000)\n",
      "> Windowing\n",
      "125 7500 60\n",
      "> Resampling\n"
     ]
    }
   ],
   "source": [
    "print(f'Preprocessing commencing...')\n",
    "print(f'> Filtering')\n",
    "taps = signal.firwin(numtaps=400, cutoff=[0.5, 8.0], window='hamming', pass_zero=False, fs=125)\n",
    "w, h = signal.freqz(taps)\n",
    "\n",
    "pool = multiprocessing.Pool(processes=40)\n",
    "syn_filtered_pleth = pool.starmap(signal.filtfilt, [(taps, 1.0, p) for p in syn_pleth125])\n",
    "pool.close()\n",
    "pool.join()\n",
    "syn_filtered_pleth = np.asarray(syn_filtered_pleth)\n",
    "\n",
    "print(syn_filtered_pleth.shape)\n",
    "\n",
    "\n",
    "print(f'> Windowing')\n",
    "fs = 125\n",
    "dataset = []\n",
    "shift_factor = 60\n",
    "window_size = fs * 60 # 125*60=7500\n",
    "shift = int(window_size/shift_factor) # 125\n",
    "samples_len = len(syn_filtered_pleth) # 196    \n",
    "\n",
    "shift_n_times = int((syn_filtered_pleth.shape[1]-window_size)/shift)\n",
    "\n",
    "print(shift, window_size, shift_n_times)\n",
    "for j in range(samples_len):\n",
    "    sliced_pleth = [syn_filtered_pleth[j][0+(shift*i):window_size+(shift*i)] for i in range(shift_n_times)]\n",
    "    matched_resp = [syn_resp[j]] * shift_n_times\n",
    "\n",
    "    for i in range(shift_n_times):\n",
    "        temp = []\n",
    "        temp.append(sliced_pleth[i])\n",
    "        temp.append(matched_resp[i])\n",
    "        dataset.append(temp)\n",
    "\n",
    "\n",
    "print(f'> Resampling')\n",
    "pool = multiprocessing.Pool(processes=40)\n",
    "result = pool.starmap(signal_resample, [(pleth[0], 125, 30) for pleth in dataset])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "syn_new_patient = []\n",
    "for i in range(len(dataset)):\n",
    "    resp = dataset[i][1]\n",
    "    resampled_pleth = result[i]\n",
    "    temp = []\n",
    "    temp.append(resampled_pleth)\n",
    "    temp.append(resp)\n",
    "    syn_new_patient.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Sampling\n",
      "Train-Test Split into 80:20 (Random seed 42)\n",
      "[0, 1, 2, 5, 6, 7, 8, 9, 10, 12, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 30, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 72, 73, 74, 76, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99] [81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71]\n"
     ]
    }
   ],
   "source": [
    "subjects = pd.read_csv(f'{DATA_PATH}/patients.csv')\n",
    "patients = subjects.loc[subjects['diagnosis']!='0']\n",
    "\n",
    "print('Init Sampling')\n",
    "rand_idx = list(range(100))\n",
    "random.seed(42)\n",
    "test_idx = random.sample(rand_idx, k=20)\n",
    "train_idx = list(set(rand_idx) - set(test_idx))\n",
    "train_patients = patients.iloc[train_idx]\n",
    "test_patients = patients.iloc[test_idx]\n",
    "print('Train-Test Split into 80:20 (Random seed 42)')\n",
    "print(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=25),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th K-fold\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Preprocessing finished: 4176 / 1048\n",
      "Prepare modeling\n",
      "(15696, 1800) (15696,)\n",
      "(15696, 1800, 1) <class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 15:52:39.071475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14353 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:73:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare modeling\n",
      "(1048, 1800) (1048,)\n",
      "(1048, 1800, 1) <class 'numpy.float32'>\n",
      "2th K-fold\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Preprocessing finished: 4157 / 1067\n",
      "Prepare modeling\n",
      "(15677, 1800) (15677,)\n",
      "(15677, 1800, 1) <class 'numpy.float32'>\n",
      "Prepare modeling\n",
      "(1067, 1800) (1067,)\n",
      "(1067, 1800, 1) <class 'numpy.float32'>\n",
      "3th K-fold\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Preprocessing finished: 4218 / 1006\n",
      "Prepare modeling\n",
      "(15738, 1800) (15738,)\n",
      "(15738, 1800, 1) <class 'numpy.float32'>\n",
      "Prepare modeling\n",
      "(1006, 1800) (1006,)\n",
      "(1006, 1800, 1) <class 'numpy.float32'>\n",
      "4th K-fold\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Preprocessing finished: 4150 / 1074\n",
      "Prepare modeling\n",
      "(15670, 1800) (15670,)\n",
      "(15670, 1800, 1) <class 'numpy.float32'>\n",
      "Prepare modeling\n",
      "(1074, 1800) (1074,)\n",
      "(1074, 1800, 1) <class 'numpy.float32'>\n",
      "5th K-fold\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n",
      "Preprocessing finished: 4195 / 1029\n",
      "Prepare modeling\n",
      "(15715, 1800) (15715,)\n",
      "(15715, 1800, 1) <class 'numpy.float32'>\n",
      "Prepare modeling\n",
      "(1029, 1800) (1029,)\n",
      "(1029, 1800, 1) <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# 7m 9s\n",
    "counter = 1\n",
    "raw_dataset = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "dataset = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "for train_idx, val_idx in kf.split(train_patients):\n",
    "    print(f'{counter}th K-fold')\n",
    "    counter = counter + 1\n",
    "    # 이렇게 하는 이유는 Train과 Validation을 완벽히 구별시키기 위함이다.\n",
    "    X_train = preprocessing(train_patients.iloc[train_idx])\n",
    "    X_val = preprocessing(train_patients.iloc[val_idx])\n",
    "    print(f'Preprocessing finished: {len(X_train)} / {len(X_val)}')\n",
    "\n",
    "    X_train.extend(syn_new_patient) # Synthetic과 SICU를 섞는다.\n",
    "    random.seed(42)\n",
    "    random.shuffle(X_train)\n",
    "\n",
    "    raw_dataset['train'].append(X_train)\n",
    "    raw_dataset['val'].append(X_val)\n",
    "    \n",
    "    train_dataset = prepare_modeling(X_train, batchsize=256)\n",
    "    val_dataset = prepare_modeling(X_val, batchsize=256)\n",
    "\n",
    "    dataset['train'].append(train_dataset)\n",
    "    dataset['val'].append(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU Avaliable: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Dense, BatchNormalization, Activation, Add, Flatten, Dropout\n",
    "print(f'Is GPU Avaliable: {tf.config.list_physical_devices(\"GPU\")}')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "class RespBlock(Model):\n",
    "    def __init__(self, filters, *args, **kwargs):\n",
    "        super(RespBlock, self).__init__(*args, **kwargs)\n",
    "        self.conv1 = Conv1D(filters=filters, kernel_size=3, strides=1, dilation_rate=1, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "\n",
    "        self.conv2 = Conv1D(filters=filters, kernel_size=3, strides=1, dilation_rate=2, padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "\n",
    "        self.conv3 = Conv1D(filters=filters, kernel_size=3, strides=1, dilation_rate=3, padding='same')\n",
    "        self.bn3 = BatchNormalization()\n",
    "\n",
    "        # self.conv4 = Conv1D(filters=filters, kernel_size=3, strides=1, dilation_rate=5, padding='same')\n",
    "        # self.bn4 = BatchNormalization()\n",
    "\n",
    "        self.conv1x1 = Conv1D(filters=filters*2, kernel_size=1, strides=1, padding='same')\n",
    "        self.bn1x1 = BatchNormalization()\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x1 = self.conv1(inputs)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = Activation('relu')(x1)\n",
    "\n",
    "        x2 = self.conv2(inputs)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = Activation('relu')(x2)\n",
    "        \n",
    "        x3 = self.conv3(inputs)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = Activation('relu')(x3)\n",
    "        \n",
    "        # x4 = self.conv4(inputs)\n",
    "        # x4 = self.bn4(x4)\n",
    "        # x4 = Activation('relu')(x4)\n",
    "\n",
    "        x = Add()([x1, x2, x3])\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.bn1x1(x)\n",
    "\n",
    "        if inputs.shape[-1] != 1:\n",
    "            inputs = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "           \n",
    "        x = Add()([x, inputs])\n",
    "        return x\n",
    "\n",
    "\n",
    "class RespDNN2(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(RespDNN2, self).__init__(*args, **kwargs)\n",
    "        self.respblk = [RespBlock(32*i) for i in np.arange(1, 4)]\n",
    "        self.dwnsamp = [Conv1D(32*i, kernel_size=3, strides=2, padding='same') for i in np.arange(1, 4)]\n",
    "        self.bn = [BatchNormalization() for _ in range(3)]\n",
    "        self.avgpool = AveragePooling1D(strides=2, padding='valid')\n",
    "        self.dense1 = Dense(1000, activation='relu')\n",
    "        self.dense2 = Dense(100, activation='relu')\n",
    "        self.dense3 = Dense(1)\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = inputs\n",
    "        for i in range(3):\n",
    "           x = self.respblk[i](x)\n",
    "           x = self.dwnsamp[i](x)\n",
    "           x = self.bn[i](x)\n",
    "           x = Activation('relu')(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th K-fold\n",
      "Epoch 1/1000\n",
      "62/62 [==============================] - 24s 192ms/step - loss: 6.4894 - mean_absolute_error: 6.4894 - val_loss: 13.4042 - val_mean_absolute_error: 13.4042 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 3.3497 - mean_absolute_error: 3.3497 - val_loss: 9.2931 - val_mean_absolute_error: 9.2931 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 2.5692 - mean_absolute_error: 2.5692 - val_loss: 7.2182 - val_mean_absolute_error: 7.2182 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 2.1960 - mean_absolute_error: 2.1960 - val_loss: 4.7753 - val_mean_absolute_error: 4.7753 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 2.0467 - mean_absolute_error: 2.0467 - val_loss: 5.4661 - val_mean_absolute_error: 5.4661 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.8910 - mean_absolute_error: 1.8910 - val_loss: 4.8284 - val_mean_absolute_error: 4.8284 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.6774 - mean_absolute_error: 1.6774 - val_loss: 5.6002 - val_mean_absolute_error: 5.6002 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.5575 - mean_absolute_error: 1.5575 - val_loss: 4.1548 - val_mean_absolute_error: 4.1548 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.3027 - mean_absolute_error: 1.3027 - val_loss: 3.6925 - val_mean_absolute_error: 3.6925 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.3681 - mean_absolute_error: 1.3681 - val_loss: 2.7611 - val_mean_absolute_error: 2.7611 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.3998 - mean_absolute_error: 1.3998 - val_loss: 2.4632 - val_mean_absolute_error: 2.4632 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4430 - mean_absolute_error: 1.4430 - val_loss: 2.4341 - val_mean_absolute_error: 2.4341 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1354 - mean_absolute_error: 1.1354 - val_loss: 2.7477 - val_mean_absolute_error: 2.7477 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0242 - mean_absolute_error: 1.0242 - val_loss: 2.9825 - val_mean_absolute_error: 2.9825 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.8977 - mean_absolute_error: 0.8977 - val_loss: 2.7754 - val_mean_absolute_error: 2.7754 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.9078 - mean_absolute_error: 0.9078 - val_loss: 2.9761 - val_mean_absolute_error: 2.9761 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9036 - mean_absolute_error: 0.9036 - val_loss: 2.9388 - val_mean_absolute_error: 2.9388 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0505 - mean_absolute_error: 1.0505 - val_loss: 2.8342 - val_mean_absolute_error: 2.8342 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9689 - mean_absolute_error: 0.9689 - val_loss: 3.1846 - val_mean_absolute_error: 3.1846 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0861 - mean_absolute_error: 1.0861 - val_loss: 3.1984 - val_mean_absolute_error: 3.1984 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8997 - mean_absolute_error: 0.8997 - val_loss: 2.9327 - val_mean_absolute_error: 2.9327 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7523 - mean_absolute_error: 0.7523 - val_loss: 3.0965 - val_mean_absolute_error: 3.0965 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7577 - mean_absolute_error: 0.7577 - val_loss: 3.2169 - val_mean_absolute_error: 3.2169 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7641 - mean_absolute_error: 0.7641 - val_loss: 3.0018 - val_mean_absolute_error: 3.0018 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8609 - mean_absolute_error: 0.8609 - val_loss: 3.2145 - val_mean_absolute_error: 3.2145 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8990 - mean_absolute_error: 0.8990 - val_loss: 3.0229 - val_mean_absolute_error: 3.0229 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0345 - mean_absolute_error: 1.0345 - val_loss: 3.0276 - val_mean_absolute_error: 3.0276 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6077 - mean_absolute_error: 0.6077 - val_loss: 2.8745 - val_mean_absolute_error: 2.8745 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5043 - mean_absolute_error: 0.5043 - val_loss: 2.7927 - val_mean_absolute_error: 2.7927 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4828 - mean_absolute_error: 0.4828 - val_loss: 2.7477 - val_mean_absolute_error: 2.7477 - lr: 1.0000e-04\n",
      "Epoch 31/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4662 - mean_absolute_error: 0.4662 - val_loss: 2.7538 - val_mean_absolute_error: 2.7538 - lr: 1.0000e-04\n",
      "Epoch 32/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4496 - mean_absolute_error: 0.4496 - val_loss: 2.7666 - val_mean_absolute_error: 2.7666 - lr: 1.0000e-04\n",
      "Epoch 33/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4439 - mean_absolute_error: 0.4439 - val_loss: 2.7748 - val_mean_absolute_error: 2.7748 - lr: 1.0000e-04\n",
      "Epoch 34/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4370 - mean_absolute_error: 0.4370 - val_loss: 2.7348 - val_mean_absolute_error: 2.7348 - lr: 1.0000e-04\n",
      "Epoch 35/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4268 - mean_absolute_error: 0.4268 - val_loss: 2.7175 - val_mean_absolute_error: 2.7175 - lr: 1.0000e-04\n",
      "Epoch 36/1000\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.4191 - mean_absolute_error: 0.4191 - val_loss: 2.7180 - val_mean_absolute_error: 2.7180 - lr: 1.0000e-04\n",
      "Epoch 37/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4064 - mean_absolute_error: 0.4064 - val_loss: 2.7311 - val_mean_absolute_error: 2.7311 - lr: 1.0000e-04\n",
      "Epoch 38/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3978 - mean_absolute_error: 0.3978 - val_loss: 2.7443 - val_mean_absolute_error: 2.7443 - lr: 1.0000e-04\n",
      "Epoch 39/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3918 - mean_absolute_error: 0.3918 - val_loss: 2.6988 - val_mean_absolute_error: 2.6988 - lr: 1.0000e-04\n",
      "Epoch 40/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3839 - mean_absolute_error: 0.3839 - val_loss: 2.6967 - val_mean_absolute_error: 2.6967 - lr: 1.0000e-04\n",
      "Epoch 41/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3837 - mean_absolute_error: 0.3837 - val_loss: 2.7253 - val_mean_absolute_error: 2.7253 - lr: 1.0000e-04\n",
      "Epoch 42/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3906 - mean_absolute_error: 0.3906 - val_loss: 2.6965 - val_mean_absolute_error: 2.6965 - lr: 1.0000e-04\n",
      "2th K-fold\n",
      "Epoch 1/1000\n",
      "62/62 [==============================] - 24s 194ms/step - loss: 5.6999 - mean_absolute_error: 5.6999 - val_loss: 8.7125 - val_mean_absolute_error: 8.7125 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 3.0118 - mean_absolute_error: 3.0118 - val_loss: 5.8497 - val_mean_absolute_error: 5.8497 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 2.7827 - mean_absolute_error: 2.7827 - val_loss: 4.7907 - val_mean_absolute_error: 4.7907 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 2.0507 - mean_absolute_error: 2.0507 - val_loss: 5.4933 - val_mean_absolute_error: 5.4933 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.7696 - mean_absolute_error: 1.7696 - val_loss: 4.9738 - val_mean_absolute_error: 4.9738 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.9414 - mean_absolute_error: 1.9414 - val_loss: 5.1526 - val_mean_absolute_error: 5.1526 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.6081 - mean_absolute_error: 1.6081 - val_loss: 5.1299 - val_mean_absolute_error: 5.1299 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4159 - mean_absolute_error: 1.4159 - val_loss: 5.3281 - val_mean_absolute_error: 5.3281 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.2112 - mean_absolute_error: 1.2112 - val_loss: 5.6293 - val_mean_absolute_error: 5.6293 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.3488 - mean_absolute_error: 1.3488 - val_loss: 4.9162 - val_mean_absolute_error: 4.9162 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.2558 - mean_absolute_error: 1.2558 - val_loss: 4.8870 - val_mean_absolute_error: 4.8870 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0092 - mean_absolute_error: 1.0092 - val_loss: 5.3403 - val_mean_absolute_error: 5.3403 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1526 - mean_absolute_error: 1.1526 - val_loss: 5.9191 - val_mean_absolute_error: 5.9191 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1128 - mean_absolute_error: 1.1128 - val_loss: 5.7705 - val_mean_absolute_error: 5.7705 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1585 - mean_absolute_error: 1.1585 - val_loss: 6.3691 - val_mean_absolute_error: 6.3691 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8479 - mean_absolute_error: 0.8479 - val_loss: 6.2369 - val_mean_absolute_error: 6.2369 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9134 - mean_absolute_error: 0.9134 - val_loss: 5.3280 - val_mean_absolute_error: 5.3280 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9986 - mean_absolute_error: 0.9986 - val_loss: 6.4833 - val_mean_absolute_error: 6.4833 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5949 - mean_absolute_error: 0.5949 - val_loss: 5.1600 - val_mean_absolute_error: 5.1600 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5300 - mean_absolute_error: 0.5300 - val_loss: 5.0875 - val_mean_absolute_error: 5.0875 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5091 - mean_absolute_error: 0.5091 - val_loss: 5.0496 - val_mean_absolute_error: 5.0496 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4921 - mean_absolute_error: 0.4921 - val_loss: 5.0618 - val_mean_absolute_error: 5.0618 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4802 - mean_absolute_error: 0.4802 - val_loss: 4.9914 - val_mean_absolute_error: 4.9914 - lr: 1.0000e-04\n",
      "Epoch 24/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4686 - mean_absolute_error: 0.4686 - val_loss: 4.9662 - val_mean_absolute_error: 4.9662 - lr: 1.0000e-04\n",
      "Epoch 25/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4620 - mean_absolute_error: 0.4620 - val_loss: 5.0092 - val_mean_absolute_error: 5.0092 - lr: 1.0000e-04\n",
      "Epoch 26/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4532 - mean_absolute_error: 0.4532 - val_loss: 5.0538 - val_mean_absolute_error: 5.0538 - lr: 1.0000e-04\n",
      "Epoch 27/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4510 - mean_absolute_error: 0.4510 - val_loss: 5.0312 - val_mean_absolute_error: 5.0312 - lr: 1.0000e-04\n",
      "Epoch 28/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4434 - mean_absolute_error: 0.4434 - val_loss: 5.0114 - val_mean_absolute_error: 5.0114 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4371 - mean_absolute_error: 0.4371 - val_loss: 5.0202 - val_mean_absolute_error: 5.0202 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4317 - mean_absolute_error: 0.4317 - val_loss: 4.9815 - val_mean_absolute_error: 4.9815 - lr: 1.0000e-04\n",
      "Epoch 31/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4175 - mean_absolute_error: 0.4175 - val_loss: 4.9956 - val_mean_absolute_error: 4.9956 - lr: 1.0000e-04\n",
      "Epoch 32/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4112 - mean_absolute_error: 0.4112 - val_loss: 5.0152 - val_mean_absolute_error: 5.0152 - lr: 1.0000e-04\n",
      "Epoch 33/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4052 - mean_absolute_error: 0.4052 - val_loss: 5.0104 - val_mean_absolute_error: 5.0104 - lr: 1.0000e-04\n",
      "3th K-fold\n",
      "Epoch 1/1000\n",
      "62/62 [==============================] - 23s 198ms/step - loss: 6.3327 - mean_absolute_error: 6.3327 - val_loss: 13.3220 - val_mean_absolute_error: 13.3220 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 3.0785 - mean_absolute_error: 3.0785 - val_loss: 7.2566 - val_mean_absolute_error: 7.2566 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 2.6246 - mean_absolute_error: 2.6246 - val_loss: 4.9113 - val_mean_absolute_error: 4.9113 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.9723 - mean_absolute_error: 1.9723 - val_loss: 4.9197 - val_mean_absolute_error: 4.9197 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.7816 - mean_absolute_error: 1.7816 - val_loss: 6.2435 - val_mean_absolute_error: 6.2435 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.7343 - mean_absolute_error: 1.7343 - val_loss: 6.8172 - val_mean_absolute_error: 6.8172 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4601 - mean_absolute_error: 1.4601 - val_loss: 6.1370 - val_mean_absolute_error: 6.1370 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.3474 - mean_absolute_error: 1.3474 - val_loss: 6.4761 - val_mean_absolute_error: 6.4761 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.3374 - mean_absolute_error: 1.3374 - val_loss: 5.3752 - val_mean_absolute_error: 5.3752 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.2901 - mean_absolute_error: 1.2901 - val_loss: 4.6679 - val_mean_absolute_error: 4.6679 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.2505 - mean_absolute_error: 1.2505 - val_loss: 4.7649 - val_mean_absolute_error: 4.7649 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1782 - mean_absolute_error: 1.1782 - val_loss: 4.0126 - val_mean_absolute_error: 4.0126 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0998 - mean_absolute_error: 1.0998 - val_loss: 4.1488 - val_mean_absolute_error: 4.1488 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0417 - mean_absolute_error: 1.0417 - val_loss: 4.3158 - val_mean_absolute_error: 4.3158 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8995 - mean_absolute_error: 0.8995 - val_loss: 3.8596 - val_mean_absolute_error: 3.8596 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9478 - mean_absolute_error: 0.9478 - val_loss: 3.6643 - val_mean_absolute_error: 3.6643 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0619 - mean_absolute_error: 1.0619 - val_loss: 3.0929 - val_mean_absolute_error: 3.0929 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.3681 - mean_absolute_error: 1.3681 - val_loss: 3.7532 - val_mean_absolute_error: 3.7532 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.0061 - mean_absolute_error: 1.0061 - val_loss: 3.5750 - val_mean_absolute_error: 3.5750 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1902 - mean_absolute_error: 1.1902 - val_loss: 3.8050 - val_mean_absolute_error: 3.8050 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1556 - mean_absolute_error: 1.1556 - val_loss: 3.7071 - val_mean_absolute_error: 3.7071 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7331 - mean_absolute_error: 0.7331 - val_loss: 3.7476 - val_mean_absolute_error: 3.7476 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6913 - mean_absolute_error: 0.6913 - val_loss: 3.6479 - val_mean_absolute_error: 3.6479 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9562 - mean_absolute_error: 0.9562 - val_loss: 3.9064 - val_mean_absolute_error: 3.9064 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9704 - mean_absolute_error: 0.9704 - val_loss: 3.7880 - val_mean_absolute_error: 3.7880 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9854 - mean_absolute_error: 0.9854 - val_loss: 3.4323 - val_mean_absolute_error: 3.4323 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8925 - mean_absolute_error: 0.8925 - val_loss: 3.7244 - val_mean_absolute_error: 3.7244 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7598 - mean_absolute_error: 0.7598 - val_loss: 3.8378 - val_mean_absolute_error: 3.8378 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8196 - mean_absolute_error: 0.8196 - val_loss: 3.5484 - val_mean_absolute_error: 3.5484 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8481 - mean_absolute_error: 0.8481 - val_loss: 3.5695 - val_mean_absolute_error: 3.5695 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7445 - mean_absolute_error: 0.7445 - val_loss: 3.8476 - val_mean_absolute_error: 3.8476 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6854 - mean_absolute_error: 0.6854 - val_loss: 3.5979 - val_mean_absolute_error: 3.5979 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5329 - mean_absolute_error: 0.5329 - val_loss: 3.7476 - val_mean_absolute_error: 3.7476 - lr: 1.0000e-04\n",
      "Epoch 34/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5046 - mean_absolute_error: 0.5046 - val_loss: 3.7723 - val_mean_absolute_error: 3.7723 - lr: 1.0000e-04\n",
      "Epoch 35/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4916 - mean_absolute_error: 0.4916 - val_loss: 3.7811 - val_mean_absolute_error: 3.7811 - lr: 1.0000e-04\n",
      "Epoch 36/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4851 - mean_absolute_error: 0.4851 - val_loss: 3.8064 - val_mean_absolute_error: 3.8064 - lr: 1.0000e-04\n",
      "Epoch 37/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4779 - mean_absolute_error: 0.4779 - val_loss: 3.7703 - val_mean_absolute_error: 3.7703 - lr: 1.0000e-04\n",
      "Epoch 38/1000\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.4644 - mean_absolute_error: 0.4644 - val_loss: 3.8012 - val_mean_absolute_error: 3.8012 - lr: 1.0000e-04\n",
      "Epoch 39/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4574 - mean_absolute_error: 0.4574 - val_loss: 3.7960 - val_mean_absolute_error: 3.7960 - lr: 1.0000e-04\n",
      "Epoch 40/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4484 - mean_absolute_error: 0.4484 - val_loss: 3.7938 - val_mean_absolute_error: 3.7938 - lr: 1.0000e-04\n",
      "Epoch 41/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4381 - mean_absolute_error: 0.4381 - val_loss: 3.8039 - val_mean_absolute_error: 3.8039 - lr: 1.0000e-04\n",
      "Epoch 42/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4392 - mean_absolute_error: 0.4392 - val_loss: 3.8061 - val_mean_absolute_error: 3.8061 - lr: 1.0000e-04\n",
      "Epoch 43/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4388 - mean_absolute_error: 0.4388 - val_loss: 3.7990 - val_mean_absolute_error: 3.7990 - lr: 1.0000e-04\n",
      "Epoch 44/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4285 - mean_absolute_error: 0.4285 - val_loss: 3.7709 - val_mean_absolute_error: 3.7709 - lr: 1.0000e-04\n",
      "Epoch 45/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4312 - mean_absolute_error: 0.4312 - val_loss: 3.7715 - val_mean_absolute_error: 3.7715 - lr: 1.0000e-04\n",
      "Epoch 46/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4402 - mean_absolute_error: 0.4402 - val_loss: 3.7956 - val_mean_absolute_error: 3.7956 - lr: 1.0000e-04\n",
      "Epoch 47/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4488 - mean_absolute_error: 0.4488 - val_loss: 3.8101 - val_mean_absolute_error: 3.8101 - lr: 1.0000e-04\n",
      "4th K-fold\n",
      "Epoch 1/1000\n",
      "62/62 [==============================] - 22s 193ms/step - loss: 6.4293 - mean_absolute_error: 6.4293 - val_loss: 11.6768 - val_mean_absolute_error: 11.6768 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 3.5096 - mean_absolute_error: 3.5096 - val_loss: 7.5532 - val_mean_absolute_error: 7.5532 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 2.6295 - mean_absolute_error: 2.6295 - val_loss: 6.0241 - val_mean_absolute_error: 6.0241 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 2.5000 - mean_absolute_error: 2.5000 - val_loss: 5.1746 - val_mean_absolute_error: 5.1746 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.7818 - mean_absolute_error: 1.7818 - val_loss: 5.3070 - val_mean_absolute_error: 5.3070 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.5516 - mean_absolute_error: 1.5516 - val_loss: 5.2060 - val_mean_absolute_error: 5.2060 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.3596 - mean_absolute_error: 1.3596 - val_loss: 4.5085 - val_mean_absolute_error: 4.5085 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.3294 - mean_absolute_error: 1.3294 - val_loss: 4.5789 - val_mean_absolute_error: 4.5789 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4631 - mean_absolute_error: 1.4631 - val_loss: 4.4194 - val_mean_absolute_error: 4.4194 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1844 - mean_absolute_error: 1.1844 - val_loss: 4.5247 - val_mean_absolute_error: 4.5247 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.2714 - mean_absolute_error: 1.2714 - val_loss: 4.1643 - val_mean_absolute_error: 4.1643 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1147 - mean_absolute_error: 1.1147 - val_loss: 4.6671 - val_mean_absolute_error: 4.6671 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0319 - mean_absolute_error: 1.0319 - val_loss: 5.0135 - val_mean_absolute_error: 5.0135 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0919 - mean_absolute_error: 1.0919 - val_loss: 5.0478 - val_mean_absolute_error: 5.0478 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0312 - mean_absolute_error: 1.0312 - val_loss: 5.6123 - val_mean_absolute_error: 5.6123 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9098 - mean_absolute_error: 0.9098 - val_loss: 5.2416 - val_mean_absolute_error: 5.2416 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9505 - mean_absolute_error: 0.9505 - val_loss: 5.0298 - val_mean_absolute_error: 5.0298 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9712 - mean_absolute_error: 0.9712 - val_loss: 5.4681 - val_mean_absolute_error: 5.4681 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8471 - mean_absolute_error: 0.8471 - val_loss: 5.2382 - val_mean_absolute_error: 5.2382 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7532 - mean_absolute_error: 0.7532 - val_loss: 4.9298 - val_mean_absolute_error: 4.9298 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7576 - mean_absolute_error: 0.7576 - val_loss: 4.9577 - val_mean_absolute_error: 4.9577 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7534 - mean_absolute_error: 0.7534 - val_loss: 5.2544 - val_mean_absolute_error: 5.2544 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7600 - mean_absolute_error: 0.7600 - val_loss: 5.3224 - val_mean_absolute_error: 5.3224 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7432 - mean_absolute_error: 0.7432 - val_loss: 5.0086 - val_mean_absolute_error: 5.0086 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6783 - mean_absolute_error: 0.6783 - val_loss: 5.1367 - val_mean_absolute_error: 5.1367 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7063 - mean_absolute_error: 0.7063 - val_loss: 4.9563 - val_mean_absolute_error: 4.9563 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.5245 - mean_absolute_error: 0.5245 - val_loss: 4.8416 - val_mean_absolute_error: 4.8416 - lr: 1.0000e-04\n",
      "Epoch 28/1000\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.4760 - mean_absolute_error: 0.4760 - val_loss: 4.8181 - val_mean_absolute_error: 4.8181 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.4638 - mean_absolute_error: 0.4638 - val_loss: 4.8520 - val_mean_absolute_error: 4.8520 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "62/62 [==============================] - 11s 178ms/step - loss: 0.4512 - mean_absolute_error: 0.4512 - val_loss: 4.8614 - val_mean_absolute_error: 4.8614 - lr: 1.0000e-04\n",
      "Epoch 31/1000\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.4398 - mean_absolute_error: 0.4398 - val_loss: 4.8762 - val_mean_absolute_error: 4.8762 - lr: 1.0000e-04\n",
      "Epoch 32/1000\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.4328 - mean_absolute_error: 0.4328 - val_loss: 4.8781 - val_mean_absolute_error: 4.8781 - lr: 1.0000e-04\n",
      "Epoch 33/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4256 - mean_absolute_error: 0.4256 - val_loss: 4.8743 - val_mean_absolute_error: 4.8743 - lr: 1.0000e-04\n",
      "Epoch 34/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4148 - mean_absolute_error: 0.4148 - val_loss: 4.8290 - val_mean_absolute_error: 4.8290 - lr: 1.0000e-04\n",
      "Epoch 35/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4058 - mean_absolute_error: 0.4058 - val_loss: 4.8767 - val_mean_absolute_error: 4.8767 - lr: 1.0000e-04\n",
      "Epoch 36/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.3996 - mean_absolute_error: 0.3996 - val_loss: 4.8320 - val_mean_absolute_error: 4.8320 - lr: 1.0000e-04\n",
      "Epoch 37/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.3915 - mean_absolute_error: 0.3915 - val_loss: 4.8869 - val_mean_absolute_error: 4.8869 - lr: 1.0000e-04\n",
      "Epoch 38/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3888 - mean_absolute_error: 0.3888 - val_loss: 4.8613 - val_mean_absolute_error: 4.8613 - lr: 1.0000e-04\n",
      "Epoch 39/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3823 - mean_absolute_error: 0.3823 - val_loss: 4.8698 - val_mean_absolute_error: 4.8698 - lr: 1.0000e-04\n",
      "Epoch 40/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3896 - mean_absolute_error: 0.3896 - val_loss: 4.8804 - val_mean_absolute_error: 4.8804 - lr: 1.0000e-04\n",
      "Epoch 41/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.3771 - mean_absolute_error: 0.3771 - val_loss: 4.8571 - val_mean_absolute_error: 4.8571 - lr: 1.0000e-04\n",
      "5th K-fold\n",
      "Epoch 1/1000\n",
      "62/62 [==============================] - 23s 193ms/step - loss: 6.0033 - mean_absolute_error: 6.0033 - val_loss: 8.7842 - val_mean_absolute_error: 8.7842 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 3.3347 - mean_absolute_error: 3.3347 - val_loss: 5.9715 - val_mean_absolute_error: 5.9715 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 2.5474 - mean_absolute_error: 2.5474 - val_loss: 6.1516 - val_mean_absolute_error: 6.1516 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 2.1741 - mean_absolute_error: 2.1741 - val_loss: 4.2769 - val_mean_absolute_error: 4.2769 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.9102 - mean_absolute_error: 1.9102 - val_loss: 4.8424 - val_mean_absolute_error: 4.8424 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.5804 - mean_absolute_error: 1.5804 - val_loss: 4.5450 - val_mean_absolute_error: 4.5450 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4227 - mean_absolute_error: 1.4227 - val_loss: 4.4214 - val_mean_absolute_error: 4.4214 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4212 - mean_absolute_error: 1.4212 - val_loss: 3.2869 - val_mean_absolute_error: 3.2869 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.5309 - mean_absolute_error: 1.5309 - val_loss: 4.5918 - val_mean_absolute_error: 4.5918 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.7257 - mean_absolute_error: 1.7257 - val_loss: 4.6754 - val_mean_absolute_error: 4.6754 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4170 - mean_absolute_error: 1.4170 - val_loss: 4.5005 - val_mean_absolute_error: 4.5005 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.4069 - mean_absolute_error: 1.4069 - val_loss: 4.3936 - val_mean_absolute_error: 4.3936 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 1.0715 - mean_absolute_error: 1.0715 - val_loss: 5.3463 - val_mean_absolute_error: 5.3463 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9631 - mean_absolute_error: 0.9631 - val_loss: 5.3219 - val_mean_absolute_error: 5.3219 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0796 - mean_absolute_error: 1.0796 - val_loss: 6.1601 - val_mean_absolute_error: 6.1601 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.2821 - mean_absolute_error: 1.2821 - val_loss: 4.1844 - val_mean_absolute_error: 4.1844 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0541 - mean_absolute_error: 1.0541 - val_loss: 5.3085 - val_mean_absolute_error: 5.3085 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.1434 - mean_absolute_error: 1.1434 - val_loss: 4.0389 - val_mean_absolute_error: 4.0389 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 1.0403 - mean_absolute_error: 1.0403 - val_loss: 4.7163 - val_mean_absolute_error: 4.7163 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.9074 - mean_absolute_error: 0.9074 - val_loss: 4.6546 - val_mean_absolute_error: 4.6546 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7474 - mean_absolute_error: 0.7474 - val_loss: 4.6933 - val_mean_absolute_error: 4.6933 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.8030 - mean_absolute_error: 0.8030 - val_loss: 5.3709 - val_mean_absolute_error: 5.3709 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.7715 - mean_absolute_error: 0.7715 - val_loss: 5.4438 - val_mean_absolute_error: 5.4438 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.6008 - mean_absolute_error: 0.6008 - val_loss: 5.1152 - val_mean_absolute_error: 5.1152 - lr: 1.0000e-04\n",
      "Epoch 25/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.5098 - mean_absolute_error: 0.5098 - val_loss: 4.9669 - val_mean_absolute_error: 4.9669 - lr: 1.0000e-04\n",
      "Epoch 26/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4928 - mean_absolute_error: 0.4928 - val_loss: 4.9441 - val_mean_absolute_error: 4.9441 - lr: 1.0000e-04\n",
      "Epoch 27/1000\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.4783 - mean_absolute_error: 0.4783 - val_loss: 4.9825 - val_mean_absolute_error: 4.9825 - lr: 1.0000e-04\n",
      "Epoch 28/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4682 - mean_absolute_error: 0.4682 - val_loss: 4.9631 - val_mean_absolute_error: 4.9631 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4627 - mean_absolute_error: 0.4627 - val_loss: 4.9474 - val_mean_absolute_error: 4.9474 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4524 - mean_absolute_error: 0.4524 - val_loss: 4.9290 - val_mean_absolute_error: 4.9290 - lr: 1.0000e-04\n",
      "Epoch 31/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4449 - mean_absolute_error: 0.4449 - val_loss: 5.0094 - val_mean_absolute_error: 5.0094 - lr: 1.0000e-04\n",
      "Epoch 32/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4335 - mean_absolute_error: 0.4335 - val_loss: 4.9948 - val_mean_absolute_error: 4.9948 - lr: 1.0000e-04\n",
      "Epoch 33/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4250 - mean_absolute_error: 0.4250 - val_loss: 4.9741 - val_mean_absolute_error: 4.9741 - lr: 1.0000e-04\n",
      "Epoch 34/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4235 - mean_absolute_error: 0.4235 - val_loss: 5.0384 - val_mean_absolute_error: 5.0384 - lr: 1.0000e-04\n",
      "Epoch 35/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4131 - mean_absolute_error: 0.4131 - val_loss: 5.0168 - val_mean_absolute_error: 5.0168 - lr: 1.0000e-04\n",
      "Epoch 36/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4145 - mean_absolute_error: 0.4145 - val_loss: 5.0364 - val_mean_absolute_error: 5.0364 - lr: 1.0000e-04\n",
      "Epoch 37/1000\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.4100 - mean_absolute_error: 0.4100 - val_loss: 5.0837 - val_mean_absolute_error: 5.0837 - lr: 1.0000e-04\n",
      "Epoch 38/1000\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.4083 - mean_absolute_error: 0.4083 - val_loss: 4.9879 - val_mean_absolute_error: 4.9879 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in range(5):\n",
    "    print(f'{i+1}th K-fold')\n",
    "    # model = ResNet()\n",
    "    # model = ResNet34()\n",
    "    # model = Unet()\n",
    "    model = RespDNN2()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "        loss=keras.losses.MeanAbsoluteError(),\n",
    "        metrics=keras.metrics.MeanAbsoluteError()\n",
    "    )  \n",
    "\n",
    "    # callbacks.append(ModelCheckpoint(f'../models/230531-Resnet-L34-stmary-KF{i}', monitor='val_loss', save_best_only=True))\n",
    "    \n",
    "    history = model.fit(\n",
    "        dataset['train'][i],\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=dataset['val'][i]\n",
    "    )\n",
    "\n",
    "    train_losses.append(min(history.history['loss']))\n",
    "    val_losses.append(min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40055903792381287 ± 0.018425293133636254\n"
     ]
    }
   ],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "print(f'{np.mean(train_losses)} ± {np.std(train_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5537909030914308 ± 0.8294573973559567\n"
     ]
    }
   ],
   "source": [
    "val_losses = np.asarray(val_losses)\n",
    "print(f'{np.mean(val_losses)} ± {np.std(val_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocessing(test_patients)\n",
    "print(f'Preprocessing finished: {len(X_test)}')\n",
    "\n",
    "test_dataset = prepare_modeling(X_test, batchsize=BATCH_SIZE)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'call model {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
