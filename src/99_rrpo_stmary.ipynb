{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성모병원 ICU로부터 수집한 PPG-RR 데이터를 이용해 7가지 모델을 검증하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 21:40:14.748525: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-26 21:40:14.800301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 21:40:15.562290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU Avaliable: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Is GPU Avaliable: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Is GPU Avaliable: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from model_src.DilatedResNet import DilatedResNet\n",
    "from model_src.BianResnet import BianResNet\n",
    "from model_src.LSTM import VanillaLSTM, CNNLSTM, BiLSTM, BiLSTMAttn\n",
    "from model_src.RespNet import RespNet\n",
    "\n",
    "print(f'Is GPU Avaliable: {tf.config.list_physical_devices(\"GPU\")}')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "DATA_PATH = '/root/Workspace/DataLake/stMary'\n",
    "DATA_SAVE_PATH = '/root/Workspace/Project-RRpo-2ndStudy/dataset' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(x, input):\n",
    "    x0 = int(np.floor(x))\n",
    "    y0 = input[x0]\n",
    "    x1 = int(np.ceil(x))\n",
    "    y1 = input[x1]\n",
    "    y = (y1-y0)*(x-x0) + y0\n",
    "    return y\n",
    "\n",
    "\n",
    "def signal_resample(input_signal, org_fs, new_fs, method='interpolation'):\n",
    "    output_signal = []\n",
    "    new_x = np.arange(0, len(input_signal), org_fs/new_fs)\n",
    "    \n",
    "    if method == 'interpolation': \n",
    "        interp = interpolation\n",
    "\n",
    "    for x in new_x:\n",
    "        y = interp(x, input_signal)\n",
    "        output_signal.append(y)\n",
    "\n",
    "    return np.asarray(output_signal)\n",
    "\n",
    "\n",
    "def generate_dataset(arg_pleths, arg_resps, fs=125, shift_factor=4):\n",
    "    \"\"\"\n",
    "    성모병원에서 수집된 데이터의 특성상 이러한 전처리를 진행해주어야 한다.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    dataset = []\n",
    "    window_size = fs * 60 # 7500\n",
    "    shift = int(window_size/shift_factor)\n",
    "    samples_len = len(arg_pleths)\n",
    "\n",
    "    cpy_resps = copy.deepcopy(arg_resps)\n",
    "    cpy_pleths = copy.deepcopy(arg_pleths)\n",
    "\n",
    "    for j in range(samples_len):\n",
    "        rr = cpy_resps[j]; ppg = cpy_pleths[j]\n",
    "\n",
    "        rr['offset'] = (rr['offset']-rr['offset'].min())/1000\n",
    "        size_lim = int(fs * np.ceil(rr['offset'].max()))\n",
    "        ppg = ppg[:size_lim]\n",
    "        shift_n_times = int((len(ppg)-window_size)/shift)+1\n",
    "\n",
    "        samp_rr = [len(rr.loc[ (rr['offset']>=0+(int(shift/fs)*i)) & ((rr['offset']<int(window_size/fs)+(int(shift/fs)*i))) ]) for i in range(shift_n_times)]\n",
    "        samp_ppg = [ppg[0+(shift*i):window_size+(shift*i)] for i in range(shift_n_times)]\n",
    "\n",
    "        for i in range(len(samp_ppg)):\n",
    "            temp = []\n",
    "            temp.append(samp_ppg[i])\n",
    "            temp.append(samp_rr[i])\n",
    "            dataset.append(temp)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocessing(targets, numtaps, cutoff, shift_factor, org_fs, new_fs, processes):\n",
    "    print('Extract PLETH/RESP')\n",
    "    pleths = [pd.read_csv(f'{DATA_PATH}/{sid}/pleth.csv', header=None, names=['sid', 'offset', 'pleth']).pleth.values for sid in targets.id.unique()]\n",
    "    resps = [pd.read_csv(f'{DATA_PATH}/{sid}/respirationTimeline.csv', header=None, names=['sid', 'offset']) for sid in targets.id.unique()]\n",
    "\n",
    "    # Before filtering: Check NaN\n",
    "    for pleth in pleths:\n",
    "        if any(np.isnan(pleth)):\n",
    "            print('check')\n",
    "\n",
    "    # Before filtering: Convert type as np.float32\n",
    "    pleths = list(map(lambda pleth: pleth.astype(np.float32), pleths))\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Filtering')\n",
    "    taps = signal.firwin(numtaps=numtaps, cutoff=cutoff, window='hamming', pass_zero=False, fs=org_fs)\n",
    "    pool = multiprocessing.Pool(processes=processes)\n",
    "    filtered_pleths = pool.starmap(signal.filtfilt, [(taps, 1.0, pleth) for pleth in pleths])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Windowing')\n",
    "    dataset = generate_dataset(filtered_pleths, resps, shift_factor=shift_factor)\n",
    "\n",
    "\n",
    "    print('Init Preprocessing: Resampling')\n",
    "    pool = multiprocessing.Pool(processes=processes)\n",
    "    result = pool.starmap(signal_resample, [(pleth[0], org_fs, new_fs) for pleth in dataset])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    new_patient = []\n",
    "    for i in range(len(dataset)):\n",
    "        temp = []\n",
    "        temp.append(result[i])\n",
    "        temp.append(dataset[i][1])\n",
    "        new_patient.append(temp)\n",
    "\n",
    "    return new_patient\n",
    "\n",
    "\n",
    "def prepare_modeling(dataset=None, batchsize=None):\n",
    "    print(f'Prepare modeling')\n",
    "    pleths = []\n",
    "    resps = []\n",
    "    for ppg, rr in dataset:\n",
    "        pleths.append(ppg.astype(np.float32))\n",
    "        resps.append(rr)\n",
    "    pleths = np.asarray(pleths)\n",
    "    resps = np.asarray(resps)\n",
    "    print(pleths.shape, resps.shape)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_pleths = np.asarray([scaler.fit_transform(pleth.reshape(-1,1)) for pleth in pleths])\n",
    "    print(scaled_pleths.shape, type(scaled_pleths[0][0][0]))\n",
    "\n",
    "    x, y = scaled_pleths[:], resps[:]\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = pd.read_csv(f'{DATA_PATH}/patients.csv')\n",
    "patients = subjects.loc[subjects['diagnosis']!='0']\n",
    "rnd_patients = patients.sample(frac=1, random_state=42)\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract PLETH/RESP\n",
      "Init Preprocessing: Filtering\n",
      "Init Preprocessing: Windowing\n",
      "Init Preprocessing: Resampling\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ps_patients \u001b[39m=\u001b[39m preprocessing(rnd_patients, numtaps\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m, cutoff\u001b[39m=\u001b[39m[\u001b[39m0.1\u001b[39m, \u001b[39m0.4\u001b[39m], shift_factor\u001b[39m=\u001b[39m\u001b[39m60\u001b[39m, org_fs\u001b[39m=\u001b[39m\u001b[39m125\u001b[39m, new_fs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, processes\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(ps_patients\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "ps_dataset = preprocessing(rnd_patients, numtaps=2000, cutoff=[0.1, 0.4], shift_factor=60, org_fs=125, new_fs=30, processes=40)\n",
    "np.save(f'{DATA_SAVE_PATH}/230920/stmary-preprocessed.npy', np.array(ps_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_dataset = np.load(f'{DATA_SAVE_PATH}/230920/stmary-preprocessed.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare modeling\n",
      "(6508, 1800) (6508,)\n",
      "(6508, 1800, 1) <class 'numpy.float32'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 1800, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_modeling(ps_dataset, batchsize=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "cutoff = [0.1, 0.4]\n",
    "raw_dataset = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "dataset = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in kf.split(train_patients):\n",
    "    print(f'{counter}th K-fold')\n",
    "    counter = counter + 1\n",
    "    # 이렇게 하는 이유는 Train과 Validation을 완벽히 구별시키기 위함이다.\n",
    "    X_train = preprocessing(train_patients.iloc[train_idx], numtaps=2000, cutoff=cutoff, shift_factor=60, org_fs=125, new_fs=30, processes=40)\n",
    "    X_val = preprocessing(train_patients.iloc[val_idx], numtaps=2000, cutoff=cutoff, shift_factor=60, org_fs=125, new_fs=30, processes=40)\n",
    "    print(f'Preprocessing finished: {len(X_train)} / {len(X_val)}')\n",
    "\n",
    "    raw_dataset['train'].append(X_train)\n",
    "    raw_dataset['val'].append(X_val)\n",
    "    \n",
    "    # train_dataset = prepare_modeling(X_train, batchsize=256)\n",
    "    # val_dataset = prepare_Mmodeling(X_val, batchsize=256)\n",
    "\n",
    "    # dataset['train'].append(train_dataset)\n",
    "    # dataset['val'].append(val_dataset)\n",
    "\n",
    "    np.save(f'{DATA_SAVE_PATH}/230919_filt{cutoff[0]}to{cutoff[1]}_win60s_rsamp30_{counter}Fold_train.npy', X_train)\n",
    "    np.save(f'{DATA_SAVE_PATH}/230919_filt{cutoff[0]}to{cutoff[1]}_win60s_rsamp30_{counter}Fold_val.npy', X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    X_train = np.load(f'{DATA_SAVE_PATH}/230919_filt0.1to0.4_win60s_rsamp30_{counter+2}Fold_train.npy', allow_pickle=True)\n",
    "    X_val = np.load(f'{DATA_SAVE_PATH}/230919_filt0.1to0.4_win60s_rsamp30_{counter+2}Fold_val.npy', allow_pickle=True)\n",
    "    \n",
    "    train_dataset = prepare_modeling(X_train, batchsize=256)\n",
    "    val_dataset = prepare_modeling(X_val, batchsize=256)\n",
    "    \n",
    "    dataset['train'].append(train_dataset)\n",
    "    dataset['val'].append(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=33),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    BianResNet(), \n",
    "    # DilatedResNet(num_of_blocks=3, dwn_kernel_size=3, filters=32),\n",
    "    # RespNet(),\n",
    "    # VanillaLSTM(),\n",
    "    # CNNLSTM(),\n",
    "    # BiLSTM(),\n",
    "    # BiLSTMAttn(units=64, units_attn=64, dropout=0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS = ['Bian', 'DilatedResNet', 'RespNet', 'LSTM', 'CNNLSTM', 'BiLSTM', 'BiLSTMAttn']\n",
    "# MODELS = ['CNNLSTM', 'BiLSTM', 'BiLSTMAttn']\n",
    "MODELS = ['Bian']\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f'\\n{MODELS[i]}')\n",
    "    print('====='*20)\n",
    "    tmp_train_losses = []\n",
    "    tmp_val_losses = []\n",
    "    \n",
    "    for j in range(5):\n",
    "        print(f'{j+1}th K-fold')\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "            loss=keras.losses.MeanAbsoluteError(),\n",
    "            metrics=keras.metrics.MeanAbsoluteError()\n",
    "        )  \n",
    "\n",
    "        callbacks.append(ModelCheckpoint(f'../models/230920/{MODELS[i]}-230919dataset-KF{j+1}', monitor='val_loss', save_best_only=True))\n",
    "        \n",
    "        history = model.fit(\n",
    "            dataset['train'][j],\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=dataset['val'][j]\n",
    "        )\n",
    "\n",
    "        val_loss_min_ix = np.argmin(history.history['val_loss'])\n",
    "        tmp_train_losses.append(history.history['loss'][val_loss_min_ix])\n",
    "        tmp_val_losses.append(history.history['val_loss'][val_loss_min_ix])\n",
    "    \n",
    "    train_losses.append(np.asarray(tmp_train_losses))\n",
    "    val_losses.append(np.asarray(tmp_val_losses))\n",
    "    print(f'{np.mean(train_losses)} ± {np.std(train_losses)}')\n",
    "    print(f'{np.mean(val_losses)} ± {np.std(val_losses)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 해야 할 내용:\n",
    "- cutoff [0.1~0.4] 에서의 결과를 확인하고 이에 따른 논문 수정이 필요하다. 이럴 경우 Result까지 수정할 필요가 있다. 이에 대해 아래 레퍼런스들을 참고하여\n",
    "\n",
    "    + IQBAL, T., A. ELAHI, S. GANLY, W. WIJNS, et al. Photoplethysmography-Based Respiratory Rate Estimation Algorithm for Health Monitoring Applications. Journal of Medical and Biological Engineering, 2022/04/01 2022, 42(2), 242-252. [0.1-0.4]\n",
    "\n",
    "- 호흡수가 정상호흡(12~18brpm), 빠른호흡(>18brpm), 느린호흡(<12brpm) 각각에 대해서 예측 RR과 실제 RR 사이의 차이를 MAE, 백분율로 표시할 것 권장.\n",
    "\n",
    "    + 이 세 부류의 호흡에 대해서 Figure를 보여줄 것 권장.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bian = [tf.keras.models.load_model(f'../models/230920/Bian-230919dataset-KF{i+1}') for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bian[1].evaluate(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = []; val_res = []\n",
    "for i in range(5):\n",
    "    train_res.append(model_bian[i].evaluate(dataset['train'][i]))\n",
    "    val_res.append(model_bian[i].evaluate(dataset['val'][i]))\n",
    "print(f'TRAIN: {np.mean(train_res)}±{np.std(train_res)} / VAL: {np.mean(val_res)}±{np.std(val_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dil = [tf.keras.models.load_model(f'../models/230919-DilatedResNet-230919dataset-KF{i+1}') for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = []; val_res = []\n",
    "for i in range(5):\n",
    "    train_res.append(model_dil[i].evaluate(dataset['train'][i]))\n",
    "    val_res.append(model_dil[i].evaluate(dataset['val'][i]))\n",
    "print(f'TRAIN: {np.mean(train_res)}±{np.std(train_res)} / VAL: {np.mean(val_res)}±{np.std(val_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet = [tf.keras.models.load_model(f'../models/230919-RespNet-230919dataset-KF{i+1}') for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = []; val_res = []\n",
    "for i in range(5):\n",
    "    train_res.append(model_unet[i].evaluate(dataset['train'][i]))\n",
    "    val_res.append(model_unet[i].evaluate(dataset['val'][i]))\n",
    "print(f'TRAIN: {np.mean(train_res)}±{np.std(train_res)} / VAL: {np.mean(val_res)}±{np.std(val_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = [tf.keras.models.load_model(f'../models/230919-LSTM-230919dataset-KF{i+1}') for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = []; val_res = []\n",
    "for i in range(5):\n",
    "    train_res.append(model_lstm[i].evaluate(dataset['train'][i]))\n",
    "    val_res.append(model_lstm[i].evaluate(dataset['val'][i]))\n",
    "print(f'TRAIN: {np.mean(train_res)}±{np.std(train_res)} / VAL: {np.mean(val_res)}±{np.std(val_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnnlstm = [tf.keras.models.load_model(f'../models/230919-CNNLSTM-230919dataset-KF{i+1}') for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = []; val_res = []\n",
    "for i in range(5):\n",
    "    train_res.append(model_cnnlstm[i].evaluate(dataset['train'][i]))\n",
    "    val_res.append(model_cnnlstm[i].evaluate(dataset['val'][i]))\n",
    "print(f'TRAIN: {np.mean(train_res)}±{np.std(train_res)} / VAL: {np.mean(val_res)}±{np.std(val_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = []; std = []\n",
    "for i in range(5):\n",
    "    y_pred = []; y_true = [];\n",
    "    for x,y in dataset['train'][i].as_numpy_iterator():\n",
    "        y_pred.append(model_cnnlstm[0].predict(x))\n",
    "        y_true.append(y)\n",
    "    \n",
    "    y_pred2 = np.concatenate(y_pred, axis=0)\n",
    "    y_true2 = np.concatenate(y_true).reshape(-1,1)\n",
    "\n",
    "    mae.append(np.mean(np.abs(y_true2 - y_pred2)))\n",
    "    std.append(np.std(np.abs(y_true2 - y_pred2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mae)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bilstm = [tf.keras.models.load_model(f'../models/230919-BiLSTM-230919dataset-KF{i+1}') for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = []; val_res = []\n",
    "for i in range(5):\n",
    "    train_res.append(model_bilstm[i].evaluate(dataset['train'][i]))\n",
    "    val_res.append(model_bilstm[i].evaluate(dataset['val'][i]))\n",
    "print(f'TRAIN: {np.mean(train_res)}±{np.std(train_res)} / VAL: {np.mean(val_res)}±{np.std(val_res)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
